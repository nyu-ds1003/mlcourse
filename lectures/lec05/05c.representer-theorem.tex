%% LyX 2.3.4.2 created this file.  For more info, see http://www.lyx.org/.
%% Do not edit unless you really know what you are doing.
\documentclass[english,dvipsnames,aspectratio=169]{beamer}
\usepackage{mathptmx}
\usepackage{eulervm}
\usepackage[T1]{fontenc}
\usepackage[latin9]{inputenc}
\usepackage{babel}
\usepackage{amstext}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{ifthen}
\usepackage{xcolor}
\usepackage{xspace}
\usepackage{tikz}
\usetikzlibrary{tikzmark}
\usetikzlibrary{calc}
\usepackage{pgfplots}
%\pgfplotsset{compat=1.17}
\usepackage{booktabs}
\usepackage{xpatch}

\usepackage{pgfpages}
\setbeamertemplate{note page}{\pagecolor{yellow!5}\insertnote}
%\setbeameroption{hide notes} % Only slides
%\setbeameroption{show only notes} % Only notes
%\setbeameroption{show notes on second screen=right} % Both

\xpatchcmd{\itemize}
  {\def\makelabel}
  {\ifnum\@itemdepth=1\relax
     \setlength\itemsep{2ex}% separation for first level
   \else
     \ifnum\@itemdepth=2\relax
       \setlength\itemsep{1ex}% separation for second level
     \else
       \ifnum\@itemdepth=3\relax
         \setlength\itemsep{0.5ex}% separation for third level
   \fi\fi\fi\def\makelabel
  }
 {}
 {}

\ifx\hypersetup\undefined
  \AtBeginDocument{%
    \hypersetup{unicode=true,pdfusetitle,
 bookmarks=true,bookmarksnumbered=false,bookmarksopen=false,
 breaklinks=false,pdfborder={0 0 0},pdfborderstyle={},backref=false,colorlinks=true,
 allcolors=NYUPurple,urlcolor=LightPurple}
  }
\else
  \hypersetup{unicode=true,pdfusetitle,
 bookmarks=true,bookmarksnumbered=false,bookmarksopen=false,
 breaklinks=false,pdfborder={0 0 0},pdfborderstyle={},backref=false,colorlinks=true,
 allcolors=NYUPurple,urlcolor=LightPurple}
\fi

\makeatletter

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% LyX specific LaTeX commands.
%% Because html converters don't know tabularnewline
\providecommand{\tabularnewline}{\\}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Textclass specific LaTeX commands.
% this default might be overridden by plain title style
\newcommand\makebeamertitle{\frame{\maketitle}}%
% (ERT) argument for the TOC
\AtBeginDocument{%
  \let\origtableofcontents=\tableofcontents
  \def\tableofcontents{\@ifnextchar[{\origtableofcontents}{\gobbletableofcontents}}
  \def\gobbletableofcontents#1{\origtableofcontents}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% User specified LaTeX commands.
\usetheme{CambridgeUS} 
\beamertemplatenavigationsymbolsempty


% Set Color ==============================
\definecolor{NYUPurple}{RGB}{87,6,140}
\definecolor{LightPurple}{RGB}{165,11,255}


\setbeamercolor{title}{fg=NYUPurple}
\setbeamercolor{frametitle}{fg=NYUPurple}

\setbeamercolor{background canvas}{fg=NYUPurple, bg=white}
\setbeamercolor{background}{fg=black, bg=NYUPurple}

\setbeamercolor{palette primary}{fg=black, bg=gray!30!white}
\setbeamercolor{palette secondary}{fg=black, bg=gray!20!white}
\setbeamercolor{palette tertiary}{fg=gray!20!white, bg=NYUPurple}

\setbeamertemplate{headline}{}
\setbeamerfont{itemize/enumerate body}{}
\setbeamerfont{itemize/enumerate subbody}{size=\normalsize}
\setbeamerfont{itemize/enumerate subsubbody}{size=\normalsize}

\setbeamercolor{parttitle}{fg=NYUPurple}
\setbeamercolor{sectiontitle}{fg=NYUPurple}
\setbeamercolor{sectionname}{fg=NYUPurple}
\setbeamercolor{section page}{fg=NYUPurple}
%\setbeamercolor{description item}{fg=NYUPurple}
%\setbeamercolor{block title}{fg=NYUPurple}

\setbeamertemplate{blocks}[rounded][shadow=false]
\setbeamercolor{block body}{bg=normal text.bg!90!NYUPurple}
\setbeamercolor{block title}{bg=NYUPurple!30, fg=NYUPurple}



\AtBeginSection[]{
  \begin{frame}
  \vfill
  \centering
\setbeamercolor{section title}{fg=NYUPurple}
 \begin{beamercolorbox}[sep=8pt,center,shadow=true,rounded=true]{title}
    \usebeamerfont{title}\usebeamercolor[fg]{title}\insertsectionhead\par%
  \end{beamercolorbox}
  \vfill
  \end{frame}
}

\makeatother

\setlength{\parskip}{\medskipamount} 

\input ../macros

\begin{document}
\input ../rosenberg-macros

\title[DS-GA 1003]{Representer Theorem}
\author{He He}
\date{March 2, 2021}
\institute{CDS, NYU}

\makebeamertitle
\mode<article>{Just in article version}

\begin{frame}{SVM solution is in the ``span of the data''}
\begin{itemize}
\item We found the SVM dual problem can be written as:
\begin{eqnarray*}
\sup_{\alpha\in\reals^{n}} &  & \sum_{i=1}^{n}\alpha_{i}-\frac{1}{2}\sum_{i,j=1}^{n}\alpha_{i}\alpha_{j}y_{i}y_{j}x_{j}^{T}x_{i}\\
\mbox{s.t.} &  & \sum_{i=1}^{n}\alpha_{i}y_{i}=0\\
 & \quad & \alpha_{i}\in\left[0,\frac{c}{n}\right]\;i=1,\ldots,n.
\end{eqnarray*}
\end{itemize}

\begin{itemize}
\item Given dual solution $\alpha^{*}$, primal solution is $\mbox{\ensuremath{w^{*}}=\ensuremath{\sum_{i=1}^{n}\alpha_{i}^{*}y_{i}x_{i}}}$. 

\item Notice: $w^{*}$ is a linear combination of training inputs $x_{1},\ldots,x_{n}$.

\item We refer to this phenomenon by saying ``$w^{*}$ is in the\textbf{
span of the data}.''
\begin{itemize}
\item Or in math, $w^{*}\in\linspan\left(x_{1},\ldots,x_{n}\right)$.
\end{itemize}
\end{itemize}
\end{frame}
%
\begin{frame}{Ridge regression solution is in the ``span of the data''}

\begin{itemize}
\item The ridge regression solution for regularization parameter $\lambda>0$
is
\[
w^{*}=\argmin_{w\in\reals^{d}}\frac{1}{n}\sum_{i=1}^{n}\left\{ w^{T}x_{i}-y_{i}\right\} ^{2}+\lambda\|w\|_{2}^{2}.
\]


\item This has a closed form solution (Homework \#3):
\[
w^{*}=\left(X^{T}X+\lambda I\right)^{-1}X^{T}y,
\]
where $X$ is the design matrix, with $x_{1},\ldots,x_{n}$ as rows. 
\end{itemize}
\end{frame}
%
\begin{frame}{Ridge regression solution is in the ``span of the data''}

\begin{itemize}
\item Rearranging $w^{*}=\left(X^{T}X+\lambda I\right)^{-1}X^{T}y,$ we
can show that (also Homework \#3):
\begin{eqnarray*}
w^{*} & = & X^{T}\underbrace{\left(\frac{1}{\lambda}y-\frac{1}{\lambda}Xw^{*}\right)}_{\alpha^{*}}\\
 & = & X^{T}\alpha^{*}=\sum_{i=1}^{n}\alpha_{i}^{*}x_{i}.
\end{eqnarray*}
\end{itemize}

\begin{itemize}
\item So $w^{*}$ is in the span of the data.
\begin{itemize}
\item i.e. $w^{*}\in\linspan\left(x_{1},\ldots,x_{n}\right)$
\end{itemize}
\end{itemize}
\end{frame}
%
\begin{frame}{If solution is in the span of the data, we can reparameterize}
\begin{itemize}
\item The ridge regression solution for regularization parameter $\lambda>0$
is
\[
w^{*}=\argmin_{w\in\reals^{d}}\frac{1}{n}\sum_{i=1}^{n}\left\{ w^{T}x_{i}-y_{i}\right\} ^{2}+\lambda\|w\|_{2}^{2}.
\]
\end{itemize}

\begin{itemize}
\item We now know that $w^{*}\in\linspan\left(x_{1},\ldots,x_{n}\right)\subset\reals^{d}$.
\item So rather than minimizing over all of $\reals^{d}$, we can minimize
over $\linspan\left(x_{1},\ldots,x_{n}\right)$.
\[
w^{*}=\argmin_{w\in\linspan\left(x_{1},\ldots,x_{n}\right)}\frac{1}{n}\sum_{i=1}^{n}\left\{ w^{T}x_{i}-y_{i}\right\} ^{2}+\lambda\|w\|_{2}^{2}.
\]
\item Let's reparameterize the objective by replacing $w$ as a linear combination of the inputs. 
\end{itemize}
\end{frame}
%
\begin{frame}{If solution is in the span of the data, we can reparameterize}
\begin{itemize}
\item Note that for any $w\in\linspan\left(x_{1},\ldots,x_{n}\right)$,
we have $w=X^{T}\alpha$, for some $\alpha\in\reals^{n}$.
\item So let's replace $w$ with $X^{T}\alpha$ in our optimization problem:
\begin{eqnarray*}
\text{[original] }w^{*} & = & \argmin_{w\in\reals^{d}}\frac{1}{n}\sum_{i=1}^{n}\left\{ w^{T}x_{i}-y_{i}\right\} ^{2}+\lambda\|w\|_{2}^{2}\\
\text{[reparameterized] }\alpha^{*} & = & \argmin_{\alpha\in\reals^{n}}\frac{1}{n}\sum_{i=1}^{n}\left\{ \left(X^{T}\alpha\right)^{T}x_{i}-y_{i}\right\} ^{2}+\lambda\|X^{T}\alpha\|_{2}^{2}.
\end{eqnarray*}
\item To get $w^{*}$ from the reparameterized optimization problem, we
just take $w^{*}=X^{T}\alpha^{*}$.
\item We changed the dimension of our optimization variable from $d$ to
$n$. Is this useful?
\end{itemize}
\end{frame}
%
\begin{frame}{Consider very large feature spaces }

\begin{itemize}
\item Suppose we have a 300-million dimension feature space {[}very large{]}
\begin{itemize}
\item (e.g. using high order monomial interaction terms as features, as
described last lecture)
\end{itemize}
\item Suppose we have a training set of 300,000 examples {[}fairly large{]}
\item In the original formulation, we solve a 300-million dimension optimization
problem.
\item In the reparameterized formulation, we solve a 300,000-dimension optimization
problem.
\item This is why we care about when the solution is in the span
of the data.

\item This reparameterization is interesting when we have more features
than data ($d\gg n$). 
\end{itemize}
\end{frame}
%
\begin{frame}{What's next?}
\begin{itemize}
\item For SVM and ridge regression, we found that the solution is in the
span of the data.

\begin{itemize}
\item derived in two rather ad-hoc ways
\end{itemize}

\item Up next: The Representer Theorem, which shows that this ``span of
the data'' result occurs far more generally, and we prove it using
basic linear algebra.
\end{itemize}
\end{frame}

\section{Math Review: Inner Product Spaces and Hilbert Spaces}

\begin{frame}
    {Hypothesis spaces we've seen so far}
    Finite-dimensional vector space (linear functions):\\
    $$
    \sH = \pc{f\colon \sX \rightarrow \reals \mid f(x) = w^Tx, \quad w,x\in\reals^d} \;.
    $$
    To consider more complex input spaces (e.g. text, images),
    we use a feature map $\phi: \sX \rightarrow \sF$:
    $$
    \sH = \pc{f\colon \sX \rightarrow \reals \mid f(x) = w^T\phi(x)} \;.
    $$
    \begin{itemize}
        \item $\phi$ does not have to be linear.
        \item The feature space $\sF$ can be $\reals^d$ (Euclidean space) or an infinite-dimensional vector space.
        \item We would like more structure on $\sF$.
    \end{itemize}
\end{frame}

 
\begin{frame}{Inner Product Space (or ``Pre-Hilbert'' Spaces)}

An \textbf{inner product space} (over reals) is a vector space $\cv$
with an \textbf{inner product}, which is a mapping
\[
\left\langle \cdot,\cdot\right\rangle :\cv\times\cv\to\reals
\]
that has the following properties: $\forall x,y,z\in\cv$ and $a,b\in\reals$:
\begin{itemize}
\item Symmetry: $\left\langle x,y\right\rangle =\left\langle y,x\right\rangle $
\item Linearity: $\left\langle ax+by,z\right\rangle =a\left\langle x,z\right\rangle +b\left\langle y,z\right\rangle $
\item Positive-definiteness: $\left\langle x,x\right\rangle \ge0$ and $\left\langle x,x\right\rangle =0\iff x=0_{\sV}$.
\end{itemize}
To show a function $\left\langle \cdot,\cdot\right\rangle$ is an inner product, we need to check the above conditions.

    Exercise: show that $\pa{x,y} \eqdef x^Ty$ is an inner product on $\reals^d$.
\end{frame}
%
\begin{frame}{Norm from Inner Product}

Inner product is nice because it gives us notions of ``size'', ``distance'', ``angle'' in the vector space.

For an inner product space, we can ddefine a norm as
\[
\|x\| \eqdef \sqrt{\left\langle x,x\right\rangle }.
\]

\begin{example}
$\reals^{d}$ with standard Euclidean inner product is an inner product
space:
\[
\left\langle x,y\right\rangle :=x^{T}y\qquad\forall x,y\in\reals^{d}.
\]
Norm is
\[
\|x\|=\sqrt{x^{T}x}.
\]
\end{example}

\end{frame}
%
%\begin{frame}{What norms can we get from an inner product?}
%\begin{theorem}
%[Parallelogram Law] A norm $\|\cdot\|$ can be written in terms of
%an inner product on $\cv$ iff $\forall x,x'\in\cv$ 
%\[
%2\|x\|^{2}+2\|x'\|^{2}=\|x+x'\|^{2}+\|x-x'\|^{2},
%\]
%and if it can, the inner product is given by the\textbf{ polarization
%identity}
%\[
%\left\langle x,x'\right\rangle =\frac{||x||^{2}+||x'||^{2}-||x-x'||^{2}}{2}.
%\]
%
%{}
%
%\end{theorem}
%
%\begin{example}
%$\ell_{1}$ norm on $\reals^{d}$ is NOT generated by an inner product.
%{[}Exercise{]} 
%\end{example}
%
%
%{}
%
%Is $\ell_{2}$ norm on $\reals^{d}$ generated by an inner product?
%\end{frame}
%
\begin{frame}{Orthogonality (Definitions)}
\begin{definition}
Two vectors are \textbf{orthogonal} if $\left\langle x,x'\right\rangle =0$.
We denote this by $x\perp x'$.
\end{definition}


\begin{definition}
$x$ is orthogonal to a set $S$, i.e. $x\perp S$, if $x\perp s$
for all $x\in S$. 
\end{definition}

\end{frame}
%
\begin{frame}{Pythagorean Theorem}
\begin{theorem}
[Pythagorean Theorem] If $x\perp x'$, then $\|x+x'\|^{2}=\|x\|^{2}+\|x'\|^{2}.$
\end{theorem}

\begin{proof}
We have
\begin{eqnarray*}
\|x+x'\|^{2} & = & \left\langle x+x',x+x'\right\rangle \\
& = & \left\langle x,x\right\rangle +\left\langle x,x'\right\rangle +\left\langle x',x\right\rangle +\left\langle x',x'\right\rangle \\
& = & \|x\|^{2}+\|x'\|^{2}.
\end{eqnarray*}
\end{proof}
\end{frame}
%
%\begin{frame}{Projection onto a Plane (Rough Definition)}
%\begin{itemize}
%\item Choose some $x\in\cv$.
%\item Let $M$ be a subspace of inner product space $\cv$.
%\item Then $m_{0}$ is the \textbf{projection of $x$ onto $M$,}
%\begin{itemize}
%\item if $m_{0}\in M$ and is the closest point to $x$ in $M$.
%
%{}
%\end{itemize}
%\item In math: For all $m\in M$, 
%\[
%\|x-m_{0}\|\le\|x-m\|.
%\]
%\end{itemize}
%\end{frame}
%%
\begin{frame}{Hilbert Space}
\begin{itemize}
\item A pre-Hilbert space is a vector space equipped with an inner product. 

\item We need an additional technical condition for Hilbert space: \textbf{completeness}.

\item A space is \textbf{complete} if all Cauchy sequences in the space
converge to a point in the space.
\end{itemize}

\begin{definition}
A \textbf{Hilbert space} is a complete inner product space.
\end{definition}

\begin{example}
    Any finite dimensional inner produce space is a Hilbert space.
\end{example}
\end{frame}

%\begin{frame}{The Projection Theorem}
%\begin{theorem}
%[Classical Projection Theorem]
%\begin{itemize}
%\item $\ch$ a Hilbert space
%\item $M$ a closed subspace of $\ch$ (picture a hyperplane through the
%origin)
%
%{}
%\item For any $x\in\ch$, there \textbf{exists a unique }$m_{0}\in M$ for
%which 
%\[
%\|x-m_{0}\|\le\|x-m\|\;\forall m\in M.
%\]
%
%
%{}
%\item This $m_{0}$ is called the \textbf{{[}orthogonal{]} projection of
%$x$ onto $M$.}
%
%{}
%\item Furthermore, $m_{0}\in M$ is the projection of $x$ onto $M$ iff
%\[
%x-m_{0}\perp M.
%\]
%\end{itemize}
%\end{theorem}
%
%\end{frame}
%
%\begin{frame}{Projection Reduces Norm}
%
%{}
%\begin{theorem}
%Let $M$ be a closed subspace of $\ch$. For any $x\in\ch$, let $m_{0}=\proj_{M}x$
%be the projection of $x$ onto $M$. Then
%\[
%\|m_{0}\|\le\|x\|,
%\]
%with equality only when $m_{0}=x$. 
%
%{}
%
%\end{theorem}
%
%\begin{proof}
%
%\begin{eqnarray*}
%\|x\|^{2} & = & \|m_{0}+(x-m_{0})\|^{2}\;(\mbox{note: }x-m_{0}\perp m_{0}\text{ by Projection theorem})\\
% & = & \|m_{0}\|^{2}+\|x-m_{0}\|^{2}\mbox{}\ \mbox{by Pythagorean theorem}\\
%\|m_{0}\|^{2} & = & \|x\|^{2}-\|x-m_{0}\|^{2}
%\end{eqnarray*}
%Then $\|x-m_{0}\|^{2}\ge0$ implies $\|m_{0}\|^{2}\le\|x\|^{2}$.
%If $\|x-m_{0}\|^{2}=0$, then $x=m_{0}$, by definition of norm.
%\end{proof}
%
%\end{frame}

\section{The Representer Theorem}
\begin{frame}{Generalize from SVM Objective}
\begin{itemize}
\item SVM objective: 
\[
\min_{w\in\reals^{d}}\frac{1}{2}\|w\|^{2}+\frac{c}{n}\sum_{i=1}^{n}\max\left(0,1-y_{i}\left[\left\langle w,x_{i}\right\rangle \right]\right).
\]


\item \textbf{Generalized objective}: 
\[
\min_{w\in\ch}R\left(\|w\|\right)+L\left(\left\langle w,x_{1}\right\rangle ,\ldots,\left\langle w,x_{n}\right\rangle \right),
\]
where
\begin{itemize}
\item $w,x_{1},\ldots,x_{n}\in\ch$ for some Hilbert space $\ch$. (We typically
have $\ch=\reals^{d}.)$
\item $\|\cdot\|$ is the norm corresponding to the inner product of $\ch$.
(i.e. $\|w\|=\sqrt{\left\langle w,w\right\rangle }$) 
\item $R:[0,\infty)\to\reals$ is nondecreasing (\textbf{Regularization
term}), and
\item $L:\reals^{n}\to\reals$ is arbitrary (\textbf{Loss term}).
\end{itemize}
\end{itemize}
\end{frame}
%
\begin{frame}{General Objective Function for Linear Hypothesis Space (Details)}
\begin{itemize}
\item \textbf{Generalized objective}: 
\[
\min_{w\in\ch}R\left(\|w\|\right)+L\left(\left\langle w,x_{1}\right\rangle ,\ldots,\left\langle w,x_{n}\right\rangle \right)
\]
\item We can map $x_i$ to a feature space.
\item The prediction/score function $x\mapsto\left\langle w,x\right\rangle $
is linear in $w$.
\end{itemize}
\end{frame}
%
\begin{frame}{General Objective Function for Linear Hypothesis Space (Details)}
\begin{itemize}
\item \textbf{Generalized objective}: 
\[
\min_{w\in\ch}R\left(\|w\|\right)+L\left(\left\langle w,x_{1}\right\rangle ,\ldots,\left\langle w,x_{n}\right\rangle \right)
\]
\item Ridge regression and SVM are of this form. (Verify this!)
\item What if we penalize with $\lambda\|w\|_{2}$ instead of $\lambda\|w\|_{2}^{2}$?
Yes! 

\item What if we use lasso regression? No! $\ell_{1}$ norm does
not correspond to an inner product. 
\end{itemize}
\end{frame}
%
\begin{frame}{The Representer Theorem: Quick Summary}
\begin{itemize}
\item \textbf{Generalized objective}: 
\[
w^{*}=\argmin_{w\in\ch}R\left(\|w\|\right)+L\left(\left\langle w,x_{1}\right\rangle ,\ldots,\left\langle w,x_{n}\right\rangle \right)
\]
\end{itemize}

\begin{itemize}
\item Representer theorem tells us we can look for $w^{*}$ in the span
of the data:
\[
w^{*}=\argmin_{w\in\linspan\left(x_{1},\ldots,x_{n}\right)}R\left(\|w\|\right)+L\left(\left\langle w,x_{1}\right\rangle ,\ldots,\left\langle w,x_{n}\right\rangle \right).
\]
\item So we can reparameterize as before:
\[
\alpha^{*}=\argmin_{\alpha\in\reals^{n}}R\left(\left\Vert \sum_{i=1}^{n}\alpha_{i}x_{i}\right\Vert \right)+L\left(\left\langle \sum_{i=1}^{n}\alpha_{i}x_{i},x_{1}\right\rangle ,\ldots,\left\langle \sum_{i=1}^{n}\alpha_{i}x_{i},x_{n}\right\rangle \right).
\]
\item Our reparameterization trick applies much more broadly than SVM and
ridge.
\end{itemize}
\end{frame}
%
\begin{frame}{The Representer Theorem}
\begin{theorem}
[Representer Theorem] Let 
\[
J(w)=R\left(\|w\|\right)+L\left(\left\langle w,x_{1}\right\rangle ,\ldots,\left\langle w,x_{n}\right\rangle \right),
\]
where
\begin{itemize}
\item $w,x_{1},\ldots,x_{n}\in\ch$ for some Hilbert space $\ch$. (We typically
have $\ch=\reals^{d}.)$
\item $\|\cdot\|$ is the norm corresponding to the inner product of $\ch$.
(i.e. $\|w\|=\sqrt{\left\langle w,w\right\rangle }$) 
\item $R:[0,\infty)\to\reals$ is nondecreasing (\textbf{Regularization
term}), and
\item $L:\reals^{n}\to\reals$ is arbitrary (\textbf{Loss term}).
\end{itemize}
Then 
it \textbf{has a minimizer of the
form} $w^{*}=\sum_{i=1}^{n}\alpha_{i}x_{i}.$
\end{theorem}
\end{frame}
%
\begin{frame}{The Representer Theorem (Proof sketch)}
\end{frame}

%\begin{enumerate}
%\item Fix any $w\in\ch$.
%
%{}
%\item Let $w_{M}=\proj_{M}w$. 
%
%{}
%\item Residual $w-w_{M}$ is orthogonal to $x$ for all $x\in M$.
%
%{}
%\item $\left\langle w,x_{i}\right\rangle =\left\langle w_{M}+w-w_{M},x_{i}\right\rangle =\left\langle w_{M},x_{i}\right\rangle +\left\langle w-w_{M},x_{i}\right\rangle =\left\langle w_{M},x_{i}\right\rangle $
%$\forall i$.
%
%{}
%\item $L\left(\left\langle w,x_{1}\right\rangle ,\ldots,\left\langle w,x_{n}\right\rangle \right)=L\left(\left\langle w_{M},x_{1}\right\rangle ,\ldots,\left\langle w_{M},x_{n}\right\rangle \right)$.
%
%{}
%\item Projections decrease norms $\implies$ $\|w_{M}\|\le\|w\|$.
%
%{}
%\item Since $R$ is nondecreasing, $R(\|w_{M}\|)\le R(\|w\|)$.
%
%{}
%\item $J(w_{M})\le J(w)$. {[}Proves first result.{]}
%
%{}
%\item If $w^{*}$ minimizes $J(w)$, then $w_{M}^{*}=\proj_{M}w^{*}$ is
%also a minimizer, since $J(w_{M}^{*})\le J(w^{*})$.
%
%{}
%\item So $\exists\alpha$ s.t. $w_{M}^{*}=\sum_{i=1}^{n}\alpha_{i}x_{i}$
%is a minimizer of $J(w)$.\textrm{ }
%\end{enumerate}
%Q.E.D.
%\end{frame}
%
%\begin{frame}{Sufficient Condition for Existence of a Minimizer}
%\begin{theorem}
%\footnote{Thanks to \href{https://www.linkedin.com/in/mingsi-long-4ba83b30/}{Mingsi Long}
%for suggesting this nice theorem and proof.}Let 
%\[
%J(w)=R\left(\|w\|\right)+L\left(\left\langle w,x_{1}\right\rangle ,\ldots,\left\langle w,x_{n}\right\rangle \right),
%\]
%and let $M=\linspan\left(x_{1},\ldots,x_{n}\right)$. Then under the
%same conditions given in the Representer theorem, if $w_{M}^{*}$
%minimizes $J(w)$ \textbf{over the set} $M$, then $w_{M}^{*}$ minimizes
%$J(w)$ over all $\ch$.
%
%{}
%\end{theorem}
%
%\begin{itemize}
%\item One consequence of the Representer theorem only applies if $J(w)$
%has a minimizer over $\ch$. This theorem tells us that it's sufficient
%to check for a constrained minimizer of $J(w)$ over $M$. If one
%exists, then it's also an unconstrained minimizer of $J(w)$ over
%$\ch$. If there is no constrained minimizer over $M$, then $J(w)$
%has no minimizer over $\ch$ (by the Representer theorem).
%\end{itemize}
%
%{}
%\begin{itemize}
%\item Bottom Line: We can jump straight to minimizing over $M$, the ``span
%of the data''. 
%\end{itemize}
%\end{frame}
%%
%\begin{frame}{Sufficient Condition for Existence of a Minimizer (Proof)}
%\begin{enumerate}
%\item Let $w_{M}^{*}\in\argmin_{w\in M}J(w)$. {[}the constrained minimizer{]}
%
%{}
%\item Consider any $w\in\ch$.
%
%{}
%\item Let $w_{M}=\proj_{M}w$. 
%
%{}
%\item By the Representer theorem, $J(w_{M})\le J(w)$.
%
%{}
%\item $J(w_{M}^{*})\le J(w_{M})$ by definition of $w_{M}^{*}$.
%
%{}
%\item Thus for any $w\in\ch$, $J(w_{M}^{*})\le J(w)$. 
%
%{}
%\item \textrm{Therefore }$w_{M}^{*}$\textrm{ minimizes $J(w)$ over $\ch$}
%\end{enumerate}
%QED
%\end{frame}

\section{Reparameterizing our Generalized Objective Function}

\begin{frame}{Rewriting the Objective Function}
\begin{itemize}
\item Define the training score function $s:\reals^{d}\to\reals^{n}$ by
\[
s(w)=\begin{pmatrix}\left\langle w,x_{1}\right\rangle \\
\vdots\\
\left\langle w,x_{n}\right\rangle 
\end{pmatrix},
\]
which gives the \textbf{training score vector} for any $w$.
\item We can then rewrite the objective function as
\[
J(w)=R\left(\|w\|\right)+L\left(s(w)\right),
\]
where now $L:\reals^{n\times1}\to\reals$ takes a column vector as
input. 
\item This will allow us to have a slick reparameterized version...
\end{itemize}
\end{frame}
%
\begin{frame}{Reparameterize the Generalized Objective}
\begin{itemize}
\item By the Representer Theorem, it's sufficient to minimize $J(w)$ for
$w$ of the form $\sum_{i=1}^{n}\alpha_{i}x_{i}$.

\item Plugging this form into $J(w)$, we see we can just minimize
\[
J_{0}(\alpha)=R\left(\left\Vert \sum_{i=1}^{n}\alpha_{i}x_{i}\right\Vert \right)+L\left(s\left(\sum_{i=1}^{n}\alpha_{i}x_{i}\right)\right)
\]
 over $\alpha=\left(\alpha_{1},\ldots,\alpha_{n}\right)^{T}\in\reals^{n\times1}$. 
\item With some new notation, we can substantially simplify 
\begin{itemize}
\item the norm piece $\|w\|=\left\Vert \sum_{i=1}^{n}\alpha_{i}x_{i}\right\Vert $,
and
\item the score piece $s(w)=s\left(\sum_{i=1}^{n}\alpha_{i}x_{i}\right)$.
\end{itemize}
\end{itemize}
\end{frame}
%
\begin{frame}{Simplifying the Reparameterized Norm}
\begin{itemize}
\item For the norm piece $\|w\|=\left\Vert \sum_{i=1}^{n}\alpha_{i}x_{i}\right\Vert $,
we have
\begin{eqnarray*}
\|w\|^{2} & = & \left\langle w,w\right\rangle \\
 & = & \left\langle \sum_{i=1}^{n}\alpha_{i}x_{i},\sum_{j=1}^{n}\alpha_{j}x_{j}\right\rangle \\
 & = & \sum_{i,j=1}^{n}\alpha_{i}\alpha_{j}\left\langle x_{i},x_{j}\right\rangle .
\end{eqnarray*}
\item This expression involves the $n^{2}$ inner products between all pairs
of input vectors.

\item We often put those values together into a matrix (Gram/Kernel matrix).
\end{itemize}
\end{frame}
%
%\begin{frame}{The Gram Matrix}
% 
%\begin{definition}
%The \textbf{Gram matrix} of a set of points $x_{1},\ldots,x_{n}$
%in an inner product space is defined as
%\[
%K=\begin{pmatrix}\left\langle x_{i},x_{j}\right\rangle \end{pmatrix}_{i,j}=\begin{pmatrix}\left\langle x_{1},x_{1}\right\rangle  & \cdots & \left\langle x_{1},x_{n}\right\rangle \\
%\vdots & \ddots & \cdots\\
%\left\langle x_{n},x_{1}\right\rangle  & \cdots & \left\langle x_{n},x_{n}\right\rangle 
%\end{pmatrix}.
%\]
%
%{}
%\end{definition}
%
%\begin{itemize}
%\item This is the traditional definition from linear algebra.
%
%{}
%\item Later today we'll introduce the notion of a ``kernel matrix''
%\begin{itemize}
%\item The Gram matrix is a special case of a \textbf{kernel matrix }for
%the identity feature map.
%\item That's why we write $K$ for the Gram matrix instead of $G$, as done
%elsewhere.
%\end{itemize}
%
%{}
%\item NOTE: In ML, we often use Gram matrix and kernel matrix to mean the
%same thing. Don't get too hung up on the definitions.
%\end{itemize}
%\end{frame}
%
\begin{frame}{Example: Gram Matrix for the Dot Product}
\begin{itemize}
\item Consider $x_{1},\ldots,x_{n}\in\reals^{d\times1}$ with the standard
inner product $\left\langle x,x'\right\rangle =x^{T}x'$.

\item Let $X\in\reals^{n\times d}$ be the \textbf{design matrix}, which
has each input vector as a row: 
\[
X=\begin{pmatrix}-x_{1}^{T}-\\
\vdots\\
-x_{n}^{T}-
\end{pmatrix}.
\]

\item Then the Gram matrix is
\begin{eqnarray*}
K & = & \begin{pmatrix}x_{1}^{T}x_{1} & \cdots & x_{1}^{T}x_{n}\\
\vdots & \ddots & \cdots\\
x_{n}^{T}x_{1} & \cdots & x_{n}^{T}x_{n}
\end{pmatrix}=\begin{pmatrix}-x_{1}^{T}-\\
\vdots\\
-x_{n}^{T}-
\end{pmatrix}\begin{pmatrix}| & \cdots & |\\
x_{1} & \cdots & x_{n}\\
| & \cdots & |
\end{pmatrix}\\
 & = & XX^{T}
\end{eqnarray*}
\end{itemize}
\end{frame}
%
\begin{frame}{Simplifying the Reparametrized Norm}
\begin{itemize}
\item With $w=\sum_{i=1}^{n}\alpha_{i}x_{i}$, we have
\begin{eqnarray*}
\|w\|^{2} & = & \left\langle w,w\right\rangle \\
 & = & \left\langle \sum_{i=1}^{n}\alpha_{i}x_{i},\sum_{j=1}^{n}\alpha_{j}x_{j}\right\rangle \\
 & = & \sum_{i,j=1}^{n}\alpha_{i}\alpha_{j}\left\langle x_{i},x_{j}\right\rangle \\
 & = & \alpha^{T}K\alpha.
\end{eqnarray*}
\end{itemize}
\end{frame}
%
\begin{frame}{Simplifying the Training Score Vector}
\begin{itemize}
\item The score for $x_{j}$ for $w=\sum_{i=1}^{n}\alpha_{i}x_{i}$ is
\begin{eqnarray*}
\left\langle w,x_{j}\right\rangle  & = & \left\langle \sum_{i=1}^{n}\alpha_{i}x_{i},x_{j}\right\rangle =\sum_{i=1}^{n}\alpha_{i}\left\langle x_{i},x_{j}\right\rangle 
\end{eqnarray*}
\item The training score vector is
\begin{eqnarray*}
 s\left(\sum_{i=1}^{n}\alpha_{i}x_{i}\right) & = & \begin{pmatrix}\sum_{i=1}^{n}\alpha_{i}\left\langle x_{i},x_{1}\right\rangle \\
\vdots\\
\sum_{i=1}^{n}\alpha_{i}\left\langle x_{i},x_{n}\right\rangle 
\end{pmatrix}=\begin{pmatrix}\alpha_{1}\left\langle x_{1},x_{1}\right\rangle +\cdots+\alpha_{n}\left\langle x_{n},x_{1}\right\rangle \\
\vdots\\
\alpha_{1}\left\langle x_{1},x_{n}\right\rangle +\cdots+\alpha_{n}\left\langle x_{n},x_{n}\right\rangle 
\end{pmatrix}\\
 & = & \begin{pmatrix}\left\langle x_{1},x_{1}\right\rangle  & \cdots & \left\langle x_{1},x_{n}\right\rangle \\
\vdots & \ddots & \cdots\\
\left\langle x_{n},x_{1}\right\rangle  & \cdots & \left\langle x_{n},x_{n}\right\rangle 
\end{pmatrix}\begin{pmatrix}\alpha_{1}\\
\vdots\\
\alpha_{n}
\end{pmatrix}\\
& = & K\alpha
\end{eqnarray*}
\end{itemize}
\end{frame}
%
\begin{frame}{Reparameterized Objective}
\begin{itemize}
\item Putting it all together, our reparameterized objective function can
be written as
\begin{eqnarray*}
J_{0}(\alpha) & = & R\left(\left\Vert \sum_{i=1}^{n}\alpha_{i}x_{i}\right\Vert \right)+L\left(s\left(\sum_{i=1}^{n}\alpha_{i}x_{i}\right)\right)\\
 & = & R\left(\sqrt{\alpha^{T}K\alpha}\right)+L\left(K\alpha\right),
\end{eqnarray*}
which we minimize over $\alpha\in\reals^{n}$.

{}
\item \textbf{All information} needed about $x_{1},\ldots,x_{n}$ is summarized
in the Gram matrix $K$.

{}
\item We're now minimizing over $\reals^{n}$ rather than $\reals^{d}$\@.

{}
\item If $d\gg n$, this can be a big win computationally (at least once
$K$ is computed).
\end{itemize}

\end{frame}
%
\begin{frame}{Reparameterizing Predictions}
\begin{itemize}
\item Suppose we've found 
\[
\alpha^{*}\in\argmin_{\alpha\in\reals^{n}}R\left(\sqrt{\alpha^{T}K\alpha}\right)+L\left(K\alpha\right).
\]


{}
\item Then we know $w^{*}=\sum_{i=1}^{n}\alpha^{*}x_{i}$ is a solution
to
\[
\argmin_{w\in\ch}R\left(\|w\|\right)+L\left(\left\langle w,x_{1}\right\rangle ,\ldots,\left\langle w,x_{n}\right\rangle \right).
\]


{}
\item The prediction on a new point $x\in\ch$ is
\[
\hat{f}(x)=\left\langle w^{*},x\right\rangle =\sum_{i=1}^{n}\alpha_{i}^{*}\left\langle x_{i},x\right\rangle .
\]


{}
\item To make a new prediction, we may need to touch all the training inputs
$x_{1},\ldots,x_{n}$. 
\end{itemize}
\end{frame}
%
\begin{frame}{More Notation}
\begin{itemize}
\item It will be convenient to define the following column vector for any
$x\in\ch$:
\[
k_{x}=\begin{pmatrix}\left\langle x_{1},x\right\rangle \\
\vdots\\
\left\langle x_{n},x\right\rangle 
\end{pmatrix}
\]


{}
\item Then we can write our predictions on a new point $x$ as
\[
\hat{f}(x)=k_{x}^{T}\alpha^{*}
\]
\end{itemize}
\end{frame}
%
\begin{frame}{Summary So Far}
\begin{itemize}
\item Original plan: 
\begin{itemize}
\item Find $w^{*}\in\argmin_{w\in\ch}R\left(\|w\|\right)+L\left(\left\langle w,x_{1}\right\rangle ,\ldots,\left\langle w,x_{n}\right\rangle \right)$
\item Predict with $\hat{f}(x)=\left\langle w^{*},x\right\rangle $.
\end{itemize}

{}
\item We showed that the following is equivalent:
\begin{itemize}
\item Find $\alpha^{*}\in\argmin_{\alpha\in\reals^{n}}R\left(\sqrt{\alpha^{T}K\alpha}\right)+L\left(K\alpha\right)$
\item Predict with $\hat{f}(x)=k_{x}^{T}\alpha^{*}$, where
\[
K=\begin{pmatrix}\left\langle x_{1},x_{1}\right\rangle  & \cdots & \left\langle x_{1},x_{n}\right\rangle \\
\vdots & \ddots & \cdots\\
\left\langle x_{n},x_{1}\right\rangle  & \cdots & \left\langle x_{n},x_{n}\right\rangle 
\end{pmatrix}\qquad\text{and}\qquad k_{x}=\begin{pmatrix}\left\langle x_{1},x\right\rangle \\
\vdots\\
\left\langle x_{n},x\right\rangle 
\end{pmatrix}
\]
\end{itemize}

{}
\begin{itemize}
\item Every element $x\in\ch$ occurs inside an inner products with a training
input $x_{i}\in\ch$.
\end{itemize}
\end{itemize}
\end{frame}
%
\begin{frame}{Kernelization}

\begin{definition}
A method is \textbf{kernelized }if every feature vector $\psi(x)$
only appears inside an inner product with another feature vector $\psi(x')$.
This applies to both the optimization problem and the prediction function.
\end{definition}


{}
\begin{itemize}
\item Here we are using $\psi(x)=x$. Thus finding 
\[
\alpha^{*}\in\argmin_{\alpha\in\reals^{n}}R\left(\sqrt{\alpha^{T}K\alpha}\right)+L\left(K\alpha\right)
\]
 and making predictions with $\hat{f}(x)=k_{x}^{T}\alpha^{*}$ is
a \textbf{kernelization} of finding
\[
w^{*}\in\argmin_{w\in\ch}R\left(\|w\|\right)+L\left(\left\langle w,x_{1}\right\rangle ,\ldots,\left\langle w,x_{n}\right\rangle \right)
\]
 and making predictions with $\hat{f}(x)=\left\langle w^{*},x\right\rangle $.
\end{itemize}
\end{frame}
%
\begin{frame}{Summary}

\begin{itemize}
\item We used duality for SVM and bare hands
for ridge regression to find their kernelized version.
\item Our principle tool for kernelization is reparameterization by the
representer theorem.
\item Once kernelized, we can apply the kernel trick: doesn't need to represent $\phi(x)$ explicitly.
\end{itemize}

\end{frame}
%

%\section{Kernel Ridge Regression}
%\begin{frame}{Kernelizing Ridge Regression}
%\begin{itemize}
%\item Ridge Regression:
%\[
%\min_{w\in\reals^{d}}\frac{1}{n}\|Xw-y\|^{2}+\lambda\|w\|^{2}
%\]
%
%
%{}
%\item Plugging in $w=\sum_{i=1}^{n}\alpha_{i}x_{i}$, we get the kernelized
%ridge regression objective function:
%\[
%\min_{\alpha\in\reals^{n}}\frac{1}{n}\|K\alpha-y\|^{2}+\lambda\alpha^{T}K\alpha
%\]
%
%
%{}
%\item This is usually just called \textbf{kernel ridge regression}.
%\end{itemize}
%\end{frame}
%%
%\begin{frame}{Kernel Ridge Regression Solutions}
%\begin{itemize}
%\item For $\lambda>0$, the \textbf{ridge regression solution} is 
%\[
%w^{*}=(X^{T}X+\lambda I)^{-1}X^{T}y
%\]
%
%
%{}
%\item and the \textbf{kernel ridge regression solution} is
%\begin{eqnarray*}
%\alpha^{*} & = & (XX^{T}+\lambda I)^{-1}y\\
% & = & (K+\lambda I)^{-1}y
%\end{eqnarray*}
%
%
%{}
%\item (Shown in homework.)
%
%{}
%\item For ridge regression we're dealing with a $d\times d$ matrix.
%\item For kernel ridge regression we're dealing an $n\times n$ matix.
%\end{itemize}
%\end{frame}
%%
%\begin{frame}{Predictions}
%\begin{itemize}
%\item Predictions in terms of $w^{*}$:
%\[
%\hat{f}(x)=x^{T}w^{*}
%\]
%\item Predictions in terms of $\alpha^{*}$:
%\[
%\hat{f}(x)=k_{x}^{T}\alpha^{*}=\sum_{i=1}^{n}\alpha_{i}^{*}x_{i}^{T}x
%\]
%\item For kernel ridge regression, need to access all training inputs $x_{1},\ldots,x_{n}$
%to predict.
%
%{}
%\item For SVM, we may not...
%\end{itemize}
%\end{frame}
%%
%
%\section{Kernel SVM}
%\begin{frame}{Kernelized SVM (From Representer Theorem) }
%\begin{itemize}
%\item The SVM objective:
%\[
%\min_{w\in\reals^{d}}\frac{1}{2}||w||^{2}+\frac{c}{n}\sum_{i=1}^{n}\max\left(0,1-y_{i}w^{T}x_{i}\right).
%\]
%
%
%{}
%\item Plugging in $w=\sum_{i=1}^{n}\alpha_{i}x_{i}$, we get
%\[
%\min_{\alpha\in\reals^{n}}\frac{1}{2}\alpha^{T}K\alpha+\frac{c}{n}\sum_{i=1}^{n}\max\left(0,1-y_{i}\left(K\alpha\right)_{i}\right)
%\]
%
%
%{}
%\item Predictions with
%\[
%\hat{f}(x)=x^{T}w^{*}=\sum_{i=1}^{n}\alpha_{i}^{*}x_{i}^{T}x.
%\]
%
%
%{}
%\item This is one way to kernelize SVM...
%\end{itemize}
%\end{frame}
%%
%\begin{frame}{Kernelized SVM (From Lagrangian Duality) }
%\begin{itemize}
%\item Kernelized SVM from computing the Lagrangian Dual Problem:
%\begin{eqnarray*}
%\max_{\alpha\in\reals^{n}} &  & \sum_{i=1}^{n}\alpha_{i}-\frac{1}{2}\sum_{i,j=1}^{n}\alpha_{i}\alpha_{j}y_{i}y_{j}x_{j}^{T}x_{i}\\
%\mbox{s.t.} &  & \sum_{i=1}^{n}\alpha_{i}y_{i}=0\\
% & \quad & \alpha_{i}\in\left[0,\frac{c}{n}\right]\;i=1,\ldots,n.
%\end{eqnarray*}
%
%
%{}
%\item If $\alpha^{*}$ is an optimal value, then
%\[
%w^{*}=\sum_{i=1}^{n}\alpha_{i}^{*}y_{i}x_{i}\qquad\text{and}\qquad\hat{f}(x)=\sum_{i=1}^{n}\alpha_{i}^{*}y_{i}x_{i}^{T}x.
%\]
%
%
%{}
%\item Note that the prediction function is also kernelized.
%\end{itemize}
%\end{frame}
%%
%\begin{frame}{Sparsity in the Data from Complementary Slackness}
%\begin{itemize}
%\item Kernelized predictions given by
%\[
%\hat{f}(x)=\sum_{i=1}^{n}\alpha_{i}^{*}y_{i}x_{i}^{T}x.
%\]
%
%
%{}
%\item By a Lagrangian duality analysis (specifically from complementary
%slackness), we find 
%\begin{eqnarray*}
%y_{i}\hat{f}(x_{i})<1 & \implies & \alpha_{i}^{*}=\frac{c}{n}\\
%y_{i}\hat{f}(x_{i})=1 & \implies & \alpha_{i}^{*}\in\left[0,\frac{c}{n}\right]\\
%y_{i}\hat{f}(x_{i})>1 & \implies & \alpha_{i}^{*}=0
%\end{eqnarray*}
%
%
%{}
%\item So we can leave out any $x_{i}$ ``on the good side of the margin''
%($y_{i}\hat{f}(x_{i})>1$).
%
%{}
%\item $x_{i}$'s that we must keep, because $\alpha_{i}^{*}\neq0$, are
%called \textbf{support vectors}.
%\end{itemize}
%\end{frame}
%
%\section{Are we done yet?}
%\begin{frame}{Computational considerations -- we're not really done yet}
%\begin{itemize}
%\item Suppose our feature space is $\ch=\reals^{d}$.
%
%{}
%\item And we use representer theorem to kernelize.
%
%{}
%\item Get optimization problem over $\reals^{n}$ rather than over $\reals^{d}$:
%\begin{eqnarray*}
%\text{[original] }w^{*} & = & \argmin_{w\in\reals^{d}}R\left(\|w\|\right)+L\left(\left\langle w,x_{1}\right\rangle ,\ldots,\left\langle w,x_{n}\right\rangle \right)\\{}
%[\text{kernelized] }\alpha^{*} & = & \argmin_{\alpha\in\reals^{n}}R\left(\sqrt{\alpha^{T}K\alpha}\right)+L\left(K\alpha\right)
%\end{eqnarray*}
%\item This seems like a good move if $d\gg n$.
%
%{}
%\item However, there is still a hidden dependence on $d$ in the kernelized
%form -- do you see it?
%\end{itemize}
%\end{frame}
%%
%\begin{frame}{Computational considerations -- we're not really done yet}
%\begin{itemize}
%\item Get optimization problem over $\reals^{n}$ rather than over $\reals^{d}$:
%\begin{eqnarray*}
%\text{[original] }w^{*} & = & \argmin_{w\in\reals^{d}}R\left(\|w\|\right)+L\left(\left\langle w,x_{1}\right\rangle ,\ldots,\left\langle w,x_{n}\right\rangle \right)\\{}
%[\text{kernelized] }\alpha^{*} & = & \argmin_{\alpha\in\reals^{n}}R\left(\sqrt{\alpha^{T}K\alpha}\right)+L\left(K\alpha\right)
%\end{eqnarray*}
%
%
%{}
%\item For the standard inner product, $K_{ij}=\left\langle x_{i},x_{j}\right\rangle =x_{i}^{T}x_{j}$,
%where $x_{i},x_{j}\in\reals^{d}$.
%\item This is still $O(d)$, and can be too slow for huge feature spaces. 
%
%{}
%\item The essence of the ``\textbf{kernel trick}'' is getting around this
%$O(d)$ dependence.
%\end{itemize}
%\end{frame}

\end{document}
