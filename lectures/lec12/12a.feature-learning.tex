%% LyX 2.3.4.2 created this file.  For more info, see http://www.lyx.org/.
%% Do not edit unless you really know what you are doing.
\documentclass[english,dvipsnames,aspectratio=169,handout]{beamer}
\usepackage{mathptmx}
\usepackage{eulervm}
\usepackage[T1]{fontenc}
\usepackage[latin9]{inputenc}
\usepackage{babel}
\usepackage{amstext}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{ifthen}
\usepackage{xcolor}
\usepackage{xspace}
\usepackage{tikz}
\usetikzlibrary{tikzmark}
\usetikzlibrary{calc}
\usepackage{pgfplots}
%\pgfplotsset{compat=1.17}
\usepackage{booktabs}
\usepackage{xpatch}
\usepackage{multirow}
\usepackage{colortbl}
\usepackage{pgfpages}




\xpatchcmd{\itemize}
  {\def\makelabel}
  {\ifnum\@itemdepth=1\relax
     \setlength\itemsep{2ex}% separation for first level
   \else
     \ifnum\@itemdepth=2\relax
       \setlength\itemsep{1ex}% separation for second level
     \else
       \ifnum\@itemdepth=3\relax
         \setlength\itemsep{0.5ex}% separation for third level
   \fi\fi\fi\def\makelabel
  }
 {}
 {}

\ifx\hypersetup\undefined
  \AtBeginDocument{%
    \hypersetup{unicode=true,pdfusetitle,
 bookmarks=true,bookmarksnumbered=false,bookmarksopen=false,
 breaklinks=false,pdfborder={0 0 0},pdfborderstyle={},backref=false,colorlinks=true,
 allcolors=NYUPurple,urlcolor=LightPurple}
  }
\else
  \hypersetup{unicode=true,pdfusetitle,
 bookmarks=true,bookmarksnumbered=false,bookmarksopen=false,
 breaklinks=false,pdfborder={0 0 0},pdfborderstyle={},backref=false,colorlinks=true,
 allcolors=NYUPurple,urlcolor=LightPurple}
\fi

\makeatletter

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% LyX specific LaTeX commands.
%% Because html converters don't know tabularnewline
\providecommand{\tabularnewline}{\\}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Textclass specific LaTeX commands.
% this default might be overridden by plain title style
\newcommand\makebeamertitle{\frame{\maketitle}}%
% (ERT) argument for the TOC
\AtBeginDocument{%
  \let\origtableofcontents=\tableofcontents
  \def\tableofcontents{\@ifnextchar[{\origtableofcontents}{\gobbletableofcontents}}
  \def\gobbletableofcontents#1{\origtableofcontents}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% User specified LaTeX commands.
\usetheme{CambridgeUS} 
\beamertemplatenavigationsymbolsempty


% Set Color ==============================
\definecolor{NYUPurple}{RGB}{87,6,140}
\definecolor{LightPurple}{RGB}{165,11,255}


\setbeamercolor{title}{fg=NYUPurple}
\setbeamercolor{frametitle}{fg=NYUPurple}

\setbeamercolor{background canvas}{fg=NYUPurple, bg=white}
\setbeamercolor{background}{fg=black, bg=NYUPurple}

\setbeamercolor{palette primary}{fg=black, bg=gray!30!white}
\setbeamercolor{palette secondary}{fg=black, bg=gray!20!white}
\setbeamercolor{palette tertiary}{fg=gray!20!white, bg=NYUPurple}

\setbeamertemplate{headline}{}
\setbeamerfont{itemize/enumerate body}{}
\setbeamerfont{itemize/enumerate subbody}{size=\normalsize}

\setbeamercolor{parttitle}{fg=NYUPurple}
\setbeamercolor{sectiontitle}{fg=NYUPurple}
\setbeamercolor{sectionname}{fg=NYUPurple}
\setbeamercolor{section page}{fg=NYUPurple}
%\setbeamercolor{description item}{fg=NYUPurple}
%\setbeamercolor{block title}{fg=NYUPurple}

\setbeamertemplate{blocks}[rounded][shadow=false]
\setbeamercolor{block body}{bg=normal text.bg!90!NYUPurple}
\setbeamercolor{block title}{bg=NYUPurple!30, fg=NYUPurple}



\AtBeginSection[]{
  \begin{frame}
  \vfill
  \centering
\setbeamercolor{section title}{fg=NYUPurple}
 \begin{beamercolorbox}[sep=8pt,center,shadow=true,rounded=true]{title}
    \usebeamerfont{title}\usebeamercolor[fg]{title}\insertsectionhead\par%
  \end{beamercolorbox}
  \vfill
  \end{frame}
}

\makeatother

\setlength{\parskip}{\medskipamount} 

\input ../macros

\begin{document}
\input ../rosenberg-macros

%\setbeameroption{show notes on second screen}

\title[DS-GA 1003]{Feature Learning}
\author{He He \\
Slides based on Lecture
\href{https://github.com/davidrosenberg/mlcourse/blob/gh-pages/Lectures/12a.neural-networks.pdf}{12a} from David Rosenberg's course materials (\url{https://github.com/davidrosenberg/mlcourse})
}
\date{April 20, 2021}
\institute{CDS, NYU}

\makebeamertitle
\mode<article>{Just in article version}

\begin{frame}
{Today's lecture}
\begin{itemize}
\item Neural networks: huge empirical success but poor theoretical understanding
\item Key idea: representation learning
\item Optimization: backpropagation + SGD
\end{itemize}
\note[item]{Today we're going to study NN. They have been around for many decades, but research stagnated for a while since they were difficult to train given limited computation power. In the last decade, there has been a huge resurgence of interest in NN due to their incredible performance in practice.}
\note[item]{Today, we are not going to talk about various architectures and optimization methods for NN, which you can learn from a deep learning class.
Instead, we're going to cover the basic idea, representation learning, and the basic optimization algorithm for NN, backpropagation.}
\end{frame}

\section{Overview}
\subsection{Feature Learning}
\begin{frame}
{Feature engineering}
\begin{itemize}
\item Learning non-linear models in a linear form:
\begin{align}
f(x) = w^T\phi(x) .
\end{align}
\item What are possible $\phi$'s we have seen?
\pause
\begin{itemize}
\item Feature maps that define a kernel, \eg polynomials of $x$
\item Feature templates, \eg $x_i \text{ AND } x_{i-1}$
\item Basis functions, \eg (shallow) decision trees
\end{itemize}
\end{itemize}
\note[item]{How do we learn a nonlinear model in a linear form? By now we should be pretty familiar with this equation. We use nonlinear feature functions, such that our model is linear in the parameters $w$, although it's nonlinear in the inputs $x$.}
\end{frame}

\begin{frame}
{Decompose the problem}
\begin{itemize}[<+->]
\item Example:
\begin{description}[<.->]
\item[Task] Predict popularity of restaurants.
\item[Raw features] \#dishes, price, wine option, zip code, \#seats, size 
\end{description}
\item Decompose into subproblems:
\begin{itemize}[<.->]
\item ${\color{blue}h_1}(\pb{\text{\#dishes, price, wine option}}) = \text{food quality}$
\item ${\color{blue}h_2}(\pb{\text{zip code}}) = \text{walkable}$
\item ${\color{blue}h_3}(\pb{\text{\#seats, size}}) = \text{nosie}$
\end{itemize}
\item Final \textit{linear} predictor uses \textbf{intermediate features} computed by $h_i$'s:
\[
w_1\cdot\text{food quality} + w_2\cdot\text{walkable} + w_3\cdot\text{nosie}
\]
\end{itemize}
\note[item]{Let's consider this example. Recall that in the adaptive basis function model we talked about last time, we want to learn basis functions or the weak classifiers. Following the similar idea, let's try to design weak classifiers here.}
\note[item]{Each weak classifier is solving a subproblem for us, which can be considered as an intermediate step towards the final prediction problem.}
\note[item]{Given these intermediate features, we can make the final prediction using a simple linear model.}
\end{frame}

\begin{frame}
{Predefined subproblems}
\begin{center}
\def\layersep{2.5cm}
\begin{tikzpicture}[shorten >=1pt,->,draw=black!50, node distance=\layersep]
    \tikzstyle{every pin edge}=[<-,shorten <=1pt]
    \tikzstyle{neuron}=[circle,fill=black!25,minimum size=17pt,inner sep=0pt]
    \tikzstyle{input neuron}=[neuron, fill=green!50];
    \tikzstyle{output neuron}=[neuron, fill=red!50];
    \tikzstyle{hidden neuron}=[neuron, fill=blue!50];
    \tikzstyle{annot} = [text width=4em, text centered]

    % Draw the input layer nodes
    \foreach \name / \y / \text in {1/1/\#dishes, 2/2/price, 3/3/wine option, 4/4/zip code, 5/5/\#seats, 6/6/size}
    % This is the same as writing \foreach \name / \y in {1/1,2/2,3/3,4/4}
        \node[input neuron, pin=left:\text] (I-\name) at (0,-\y) {};

    % Draw the hidden layer nodes
    \foreach \name / \y in {1,...,3}
        \path[yshift=-1.3cm]
            node[hidden neuron] (H-\name) at (\layersep,-\y cm) {};

    % Draw the output layer node
    \node[output neuron,node distance=4cm,pin={[pin edge={->}]right:Popularity}, right of=H-2] (O) {};

    % Connect every node in the input layer with every node in the
    % hidden layer.
%    \foreach \source in {1,...,6}
%        \foreach \dest in {1,...,3}
%            \path (I-\source) edge (H-\dest);
	\foreach \source in {1,2,3}
		\path (I-\source) edge (H-1);
	\foreach \source in {4}
		\path (I-\source) edge (H-2);
	\foreach \source in {5,6}
		\path (I-\source) edge (H-3);

    % Connect every node in the hidden layer with the output layer
    \foreach \source in {1,...,3}
        \path (H-\source) edge (O);

    % Annotate the layers
    \node[annot,above of=H-1, node distance=2.5cm] (hl) {Intermediate features};
    \node[annot,left of=hl] {Input features};
    \node[annot,right of=hl, node distance=4cm] {Output};
    
    % Annotate the hidden nodes
    \foreach \h / \text in {1/food quality, 2/walkable, 3/noise}
    		\node[annot, above of=H-\h, node distance=0.5cm] {$h_\h$};
    \foreach \h / \text in {1/food quality, 2/walkable, 3/noise}
    		\node[annot, right of=H-\h, node distance=2cm, text width=4cm] {\text};
\end{tikzpicture}
\end{center}
\note[item]{Let's try to represent our model as a graph. So we have nodes representing each input, nodes representing the intermediate predictors which takes in a subset of inputs, and the node representing the linear classifier that computes the output.}
\note[item]{The subproblems are manually specified based on our knowledge of the problem. 
In fact, feature engineering is an important step when building practical ML models, but in this course so far we have ignored that part and assume they are given to use.
But we don't always have this knowledge, or our features aren't good, or we simply want to automate as many steps as possible.
It would be desirable if we can directly learn these subproblems.}
\end{frame}

\begin{frame}
{Learned intermediate features}
\begin{center}
\def\layersep{2.5cm}
\begin{tikzpicture}[shorten >=1pt,->,draw=black!50, node distance=\layersep]
    \tikzstyle{every pin edge}=[<-,shorten <=1pt]
    \tikzstyle{neuron}=[circle,fill=black!25,minimum size=17pt,inner sep=0pt]
    \tikzstyle{input neuron}=[neuron, fill=green!50];
    \tikzstyle{output neuron}=[neuron, fill=red!50];
    \tikzstyle{hidden neuron}=[neuron, fill=blue!50];
    \tikzstyle{annot} = [text width=4em, text centered]

    % Draw the input layer nodes
    \foreach \name / \y / \text in {1/1/\#dishes, 2/2/price, 3/3/wine option, 4/4/zip code, 5/5/\#seats, 6/6/size}
    % This is the same as writing \foreach \name / \y in {1/1,2/2,3/3,4/4}
        \node[input neuron, pin=left:\text] (I-\name) at (0,-\y) {};

    % Draw the hidden layer nodes
    \foreach \name / \y in {1,...,3}
        \path[yshift=-1.3cm]
            node[hidden neuron] (H-\name) at (\layersep,-\y cm) {};

    % Draw the output layer node
    \node[output neuron,node distance=4cm,pin={[pin edge={->}]right:Popularity}, right of=H-2] (O) {};

    % Connect every node in the input layer with every node in the
    % hidden layer.
    \foreach \source in {1,...,6}
        \foreach \dest in {1,...,3}
            \path (I-\source) edge (H-\dest);

    % Connect every node in the hidden layer with the output layer
    \foreach \source in {1,...,3}
        \path (H-\source) edge (O);

    % Annotate the layers
    \node[annot,above of=H-1, node distance=2.5cm] (hl) {\textcolor{blue}{Hidden} layer};
    \node[annot,left of=hl] {Input layer};
    \node[annot,right of=hl, node distance=4cm] {Output layer};
    
    % Annotate the hidden nodes
    \foreach \h / \text in {1/food quality, 2/walkable, 3/noise}
    		\node[annot, above of=H-\h, node distance=0.5cm] {$h_\h$};
\end{tikzpicture}
\end{center}
\note[item]{Let's re-draw our graph. Without knowing the structure of the subproblems, we'll have to give all inputs to the intermediate predictors, and hope it can learn some features automatically. But it will be hard to tell what they learned exactly, so we will call these nodes \emph{hidden units}, and they form the \emph{hidden layer}. Similarly, the input nodes form the input layer and the last layer is the output layer.}
\end{frame}

\begin{frame}
{Neural networks}
\textbf{Key idea}: automatically learn the intermediate features.
\begin{description}[Feature engineering]
\item<+->[Feature engineering]
Manually specify $\phi(x)$ based on domain knowledge and learn the weights:
\begin{align}
f(x) = {\color{red}w}^T\phi(x) .
\end{align}

\item<+->[Feature learning]
Automatically learn both the features ($K$ hidden units) and the weights:
\begin{align}
h(x) &= \pb{{\color{red}h_1}(x), \ldots, {\color{red}h_K}(x)} , \\
f(x) &= {\color{red}w}^T h(x)
\end{align}
\end{description}
\note[item]{Let's write our model down in equations now.}
\note[item]{Typically, we specify $\phi$ based on domain knowledge.}
\note[item]{NN learn both the features and the weights.}
\note[item]{But we haven't parametrized $h$ yet.}
\end{frame}

\begin{frame}
{Activation function}
\begin{itemize}[<+->]
\item How should we parametrize $h_i$'s? Can it be linear?
\onslide<+->{
\begin{align}
h_i(x) = {\color{blue}\sigma}(v_i^T x) .
\end{align}
}
\item<.-> $\sigma$ is the \emph{nonlinear} \textbf{activation function}.
\item What might be some activation functions we want to use?
\begin{itemize}
\item $\text{sign}$ function? \alarm{Non-differentiable}.
\item \emph{Differentiable} approximations: sigmoid functions.
\begin{itemize}[<.->]
\item E.g., logistic function, hyperbolic tangent function.
\end{itemize}
\end{itemize}
\item Two-layer neural network (one \textcolor{blue}{hidden layer} and one  \textcolor{Green}{output layer}) with $K$ hidden units:
\begin{align}
f(x) &= \sum_{k=1}^{K} {\color{Green}w_k} h_k(x) = \sum_{k=1}^K {\color{Green}w_k} \sigma({\color{blue}v_k}^T x)
\end{align}
\end{itemize}
\note[item]{Recall that $h$ is an intermediate predictor. How about the sign function such that $h$ would output the result of a binary classifier.}
\note[item]{Sigmoid function is S-shaped functions that approximate sign function. (draw)}
\note[item]{Let's write down our two-layer network. In the output layer, we have a linear combination of the hidden units. Each hidden units compute a linear combination of its inputs followed by an activation function.}
\note[item]{Without the activation function this is just a linear model. What do we gain from these nonlinear activation functions? Let's look at how well a two layer NN approximate different functions.}
\end{frame}

\begin{frame}{Activation Functions}

\begin{itemize}
\item The \textbf{hyperbolic tangent} is a common activation function:
\[
\sigma(x)=\tanh\left(x\right).
\]
\end{itemize}
\begin{figure}
\includegraphics[height=0.55\textheight]{figures/activationFn-Tanh}
\end{figure}
\end{frame}
%
\begin{frame}{Activation Functions}
\begin{itemize}
\item More recently, the \textbf{rectified linear} (\textbf{ReLU}) function has been very
popular:
\[
\sigma(x)=\max(0,x).
\]
\item Much \textcolor{Green}{faster} to calculate, and to calculate
its derivatives.
\item Also often seems to work better.
\end{itemize}
\begin{figure}
\includegraphics[height=0.55\textheight]{figures/activationFn-Rectified_Linear} 
\end{figure}
\end{frame}

\subsection{Approximation Properties of Two-layer Neural Networks}
\begin{frame}{Approximation Ability: $f(x)=x^{2}$}
\begin{itemize}
\item 3 hidden units; tanh activation functions 
\item Blue dots are training points; Dashed lines are hidden unit outputs;
Final output in Red.
\end{itemize}
\centering
\includegraphics[height=0.6\textheight]{{figures/Figure5.3a.pdf}} 

\let\thefootnote\relax\footnotetext{\tiny{From Bishop's \emph{Pattern Recognition and Machine Learning}, Fig 5.3 }}
\note[item]{Convex.}
\end{frame}
%
\begin{frame}{Approximation Ability: $f(x)=\sin(x)$}
\begin{itemize}
\item 3 hidden units; logistic activation function
\item Blue dots are training points; Dashed lines are hidden unit outputs;
Final output in Red.
\end{itemize}
\centering
\includegraphics[height=0.6\textheight]{{figures/Figure5.3b}.pdf} 

\let\thefootnote\relax\footnotetext{\tiny{From Bishop's \emph{Pattern Recognition and Machine Learning}, Fig 5.3 }}
\note[item]{Nonconvex but smooth.}
\end{frame}
%
\begin{frame}{Approximation Ability: $f(x)=\left|x\right|$}
\begin{itemize}
\item 3 hidden units; logistic activation functions 
\item Blue dots are training points; Dashed lines are hidden unit outputs;
Final output in Red.
\end{itemize}
\centering
\includegraphics[height=0.6\textheight]{{figures/Figure5.3c}.pdf} 

\let\thefootnote\relax\footnotetext{\tiny{From Bishop's \emph{Pattern Recognition and Machine Learning}, Fig 5.3 }}
\note[item]{Non-smooth.}
\end{frame}
%
\begin{frame}{Approximation Ability: $f(x)=\ind{x>0}$}
\begin{itemize}
\item 3 hidden units; logistic activation function
\item Blue dots are training points; Dashed lines are hidden unit outputs;
Final output in Red.
\end{itemize}
\centering
\includegraphics[height=0.6\textheight]{{figures/Figure5.3d}.pdf} 

\let\thefootnote\relax\footnotetext{\tiny{From Bishop's \emph{Pattern Recognition and Machine Learning}, Fig 5.3 }}
\note[item]{Not continuous}
\end{frame}

\begin{frame}
{Universal approximation theorems}
How much expressive power do we gain from the nonlinearity? 
\begin{theorem}[Universal approximation theorem]
A neural network with one \alarm{possibly huge hidden layer} $\hat{F}(x)$ can 
approximate \emph{any continuous function} $F(x)$ on a closed and bounded subset of $\reals^d$ under mild assumptions on the activation function, \ie $\forall \epsilon>0$, there exists an integer $N$ s.t.
\begin{align}
\hat{F}(x) = \sum_{i=1}^{\color{red}N} w_i \sigma(v_i^Tx + b_i)
\end{align}
satisfies $\vert \hat{F}(x) - F(x)\vert < \epsilon$.
\end{theorem}
\begin{itemize}
\item Number of hidden units needs to be exponential in $d$.
\item Doesn't say how to learn these parameters.
\end{itemize}
\note[item]{\footnotesize{It turns out this nonlinearity gives us a lot of additional expressive power. The universal approximation theorem says that a neural network with one hidden layer can basically approximate any continuous function arbitrarily close. The number of hidden units could be huge though, more specifically it's exponential in the input dimension $d$. You can read the more formal description offline.}}
\note[item]{Let's think about what this theorem means for a minute. Is this the reason why neural networks work so well?
First of all, there are many other universal approximates; many non-parametric methods can approximate a function arbitrarily well, \eg kernel methods, trees, basis function methods.}
\note[item]{Now you might say, but NN is parametric. But the theorem requires that the number of hidden units be exponential in $d$, what does that mean? It could remember all training examples, which could effectively make it behave like a non-parametric method such as nearest neighbors.}
\note[item]{Secondly, the theorem doesn't help us deriving a practical algorithm. We cannot have such a huge hidden layer in practice, and we don't know how to learn the parameters.}
\note[item]{So, to summarize, while the theorem holds, it doesn't provide us much guidance both theoretically and practically.}
\note[item]{Luckily, one idea that works very well in practice is to go deeper instead of wider.}
\end{frame}



\subsection{Multilayer Perceptron}
\begin{frame}
{Multilayer perceptron / Feed-forward neural networks}
\begin{itemize}
\item Wider: more hidden units.
\item Deeper: more hidden layers.
\end{itemize}
\begin{center}
\def\layersep{2.5cm}
\begin{tikzpicture}[shorten >=1pt,->,draw=black!50, node distance=\layersep]
    \tikzstyle{every pin edge}=[<-,shorten <=1pt]
    \tikzstyle{neuron}=[circle,fill=black!25,minimum size=17pt,inner sep=0pt]
    \tikzstyle{input neuron}=[neuron, fill=green!50];
    \tikzstyle{output neuron}=[neuron, fill=red!50];
    \tikzstyle{hidden neuron}=[neuron, fill=blue!50];
    \tikzstyle{annot} = [text width=4em, text centered]

    % Draw the input layer nodes
    \foreach \name / \y / \text in {1/1/1, 2/2/2, 3/3/, 4/4/d-1, 5/5/d}{
    		\ifthenelse{\y=3}
    		{\node[input neuron, pin=left:$\vdots$] (I-\name) at (0,-\y) {}}
        {\node[input neuron, pin=left:$x_{\text}$] (I-\name) at (0,-\y) {}}
	;}
    % Draw the hidden layer nodes
    \foreach \name / \y in {1,...,4}
        \path[yshift=-.5cm]
            node[hidden neuron] (H-\name) at (\layersep,-\y cm) {};
            
    % Draw the hidden layer nodes
    \foreach \name / \y in {1,...,3}
        \path[yshift=-1cm]
            node[hidden neuron] (H2-\name) at (2*\layersep,-\y cm) {};

    % Draw the output layer node
    \node[output neuron,pin={[pin edge={->}]right:score}, right of=H2-2] (O) {};

    % Connect every node in the input layer with every node in the
    % hidden layer.
    \foreach \source in {1,...,5}
        \foreach \dest in {1,...,4}
            \path (I-\source) edge (H-\dest);
    \foreach \source in {1,...,4}
        \foreach \dest in {1,...,3}
            \path (H-\source) edge (H2-\dest);

    % Connect every node in the hidden layer with the output layer
    \foreach \source in {1,...,3}
        \path (H2-\source) edge (O);

    % Annotate the layers
    \node[annot,above of=H-1, node distance=1.3cm] (hl) {{Hidden} layers};
    \node[annot,left of=hl] {Input layer};
    \node[annot,right of=hl, node distance=5cm] {Output layer};
    
\end{tikzpicture}
\end{center}
\note[item]{It's pretty easy to extend the two layer NN we just talked about. We can either add more hidden layers, or more hidden units in each layer. Modern neural networks usually have many hidden layers. If the nodes in any two adjacent layers are fully connected, we call it a MLP or FFNN.}
\end{frame}

\begin{frame}{Multilayer Perceptron: Standard Recipe}
\begin{itemize}
\item \textbf{Input space}: $\cx=\reals^{d}$$\qquad$\textbf{Action space}
$\ca=\reals^{k}$ (for $k$-class classification).

\item Let $\sigma:\reals\to\reals$ be an activation function
(e.g. $\tanh$ or ReLU).

\item Let's consider an MLP of $L$ hidden layers, each having $m$ hidden units.

\item First hidden layer is given by
\[
h^{(1)}(x)=\sigma\left(W^{(1)}x+b^{(1)}\right),
\]
for parameters $W^{(1)}\in\reals^{m\times d}$ and $b\in\reals^{m}$,
and where $\sigma\left(\cdot\right)$ is applied to each entry of
its argument.
\end{itemize}
\note[item]{Let's write down a MLP with $L$ hidden layer in math.}
\note[item]{The first layer is just the activation function composed with an affine transformation of the input.}
\note[item]{Note that $\sigma$ operates on each element of the vector.}
\end{frame}
%
\begin{frame}{Multilayer Perceptron: Standard Recipe}
\begin{itemize}[<+->]
\item Each subsequent hidden layer takes the \emph{output $o\in\reals^{m}$ of
previous layer} and produces
\[
h^{(j)}({\color{blue}o^{(j-1)}})=\sigma\left(W^{(j)}{\color{blue}o^{(j-1)}}+b^{(j)}\right),\text{ for }j=2,\ldots,L
\]
where $W^{(j)}\in\reals^{m\times m}$, $b^{(j)}\in\reals^{m}$.

\item Last layer is an \emph{affine} mapping (no activation function): 
\[
a(o^{(L)})=W^{(L+1)}o^{(L)}+b^{(L+1)},
\]
where $W^{(L+1)}\in\reals^{k\times m}$ and $b^{(L+1)}\in\reals^{k}$.

\item The full neural network function is given by the \emph{composition} of
layers:
\begin{align}
f(x) &= \left(a\circ h^{(L)}\circ\cdots\circ h^{(1)}\right)(x)
\end{align}

\item Last layer typically gives us a score. How to do classification?
\end{itemize}
\end{frame}
%

\begin{frame}{Multinomial Logistic Regression}
\begin{itemize}
\item From each $x$, we compute a linear score function for each class:
\[
x\mapsto\left(\left\langle w_{1},x\right\rangle ,\ldots,\left\langle w_{k},x\right\rangle \right)\in\reals^{k}
\]
\item We need to map this $\reals^{k}$ vector into a probability vector
$\theta$.

\pause{}
\item The \textbf{softmax function} maps scores $s=\left(s_{1},\ldots,s_{k}\right)\in\reals^{k}$
to a categorical distribution\textbf{: 
\[
\left(s_{1},\ldots,s_{k}\right)\mapsto\theta=\softmax\left(s_{1},\ldots,s_{k}\right)=\left(\frac{\exp\left(s_{1}\right)}{\sum_{i=1}^{k}\exp\left(s_{i}\right)},\ldots,\frac{\exp\left(s_{k}\right)}{\sum_{i=1}^{k}\exp\left(s_{i}\right)}\right)
\]
}
\end{itemize}
\end{frame}

\begin{frame}{Nonlinear Generalization of Multinomial Logistic Regression}
\begin{itemize}
\item From each $x$, we compute a non-linear score function for each class:
\[
x\mapsto\left(f_1(x) ,\ldots, f_k(x) \right)\in\reals^{k}
\]
where $f_i$'s are outputs of the last hidden layer of a neural network.
\item {Learning}: Maximize the log-likelihood of training data
\[
\argmax_{f_{1},\ldots,f_{k}}\sum_{i=1}^{n}\log\left[\softmax\left(f_{1}(x),\ldots,f_{k}(x)\right)_{y_{i}}\right].
\]

%\item Unfortunately, this objective function is not concave or convex. 
%
%\item But we can still use gradient methods to find a good local optimum.
\end{itemize}
\end{frame}

\begin{frame}{Neural network as a feature extractor}

\begin{itemize}
\item OverFeat is a neural network for object classification, localization, and detection.
\begin{itemize}
\item Trained on the huge ImageNet dataset
\item Lots of computing resources used for training the network.
\end{itemize}

\item All those hidden layers of the network are very valuable \emph{features}.

\begin{itemize}
\item Paper: ``\href{https://arxiv.org/pdf/1312.6229.pdf}{CNN Features off-the-shelf: an Astounding Baseline
for Recognition}''
\item Showed that using features from OverFeat makes it easy to achieve
state-of-the-art performance on new vision tasks.
\end{itemize}
\end{itemize}
\let\thefootnote\relax\footnotetext{\tiny{OverFeat code is at \url{https://github.com/sermanet/OverFeat}}}
\note[item]{In general it's not easy to tell what features have been learned by the neural network. However, sometimes the representations learned by a model can be useful for tasks the model is not trained on, which is evidence that it has learned something useful.}
\end{frame}

\begin{frame}
{Review}
We've seen
\begin{itemize}
\item Key idea: automatically discover useful features from raw data---\emph{feature/representation learning}.
\item Building blocks:
\begin{description}[Hidden layer(s)]
\item[Input layer] no learnable parameters
\item[Hidden layer(s)] perceptron + \emph{nonlinear} activation function
\item[Output layer] affine (+ transformation)
\end{description}
\item A single hidden layer is sufficient to approximate any function.
\item In practice, often have multiple hidden layers.
\end{itemize}
Next, how to learn the parameters.
\end{frame}


\end{document}
