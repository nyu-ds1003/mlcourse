{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DS-GA 1003 Machine Learning Spring 2021\n",
    "## Lab 1: 3-Feb-2020, Wednesday\n",
    "## Gradients and Directional Derivatives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, you will learn to:\n",
    "\n",
    "- use Python to compute the gradient for two simple functions\n",
    "- how to empirically check the correctness of your gradient computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 1. Computing Gradients\n",
    "\n",
    "Most numerical optimization methods require that we compute gradients of the loss function that we are attempting to minimize.  \n",
    "\n",
    "In this section, we illustrate how to compute gradients efficiently in python for a few simple examples.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Example 1: simple vector function\n",
    "\n",
    "Let's consider a simple function:\n",
    "\n",
    "$$f(\\mathbf{\\theta}) = \\theta_0^2 + 2\\theta_0 \\theta_1^3$$ \n",
    "\n",
    "The gradient of this function can be computed as following:\n",
    "\n",
    "$$\\frac{\\partial}{\\partial \\theta_0} = 2\\theta_0 + 2\\theta_1^3$$\n",
    "\n",
    "$$\\frac{\\partial}{\\partial \\theta_1} = 6\\theta_0\\theta_1^2$$\n",
    "\n",
    "$$\\nabla f(\\mathbf{\\theta}) = \\begin{bmatrix} 2\\theta_0 + 2\\theta_1^3 \\\\ 6\\theta_0\\theta_1^2 \\end{bmatrix}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def simple_vector_func(theta):\n",
    "    \"\"\"\n",
    "    Function that returns the value and gradient for a given theta vector\n",
    "    @param theta: 2d numpy array [theta_0, theta_1]\n",
    "    @out: f(\\theta), gradient of f(\\theta)\n",
    "    \"\"\"\n",
    "    # unpack vector\n",
    "    theta_0, theta_1 = theta\n",
    "    \n",
    "    # compute the value\n",
    "    y_val = np.power(theta_0,2) + 2*theta_0* np.power(theta_1,3)\n",
    "\n",
    "    # compute the gradient\n",
    "    gradient_theta0 = 2*theta_0 + 2*np.power(theta_1,3)\n",
    "    gradient_theta1 = 6*theta_0*np.power(theta_1,2)\n",
    "    gradient = np.array([gradient_theta0, gradient_theta1])\n",
    "    return y_val, gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run this function for some random $\\theta$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_val=112, grad=[ 58 108]\n"
     ]
    }
   ],
   "source": [
    "theta = np.array([2,3])\n",
    "y_val, grad = simple_vector_func(theta)\n",
    "print(\"y_val={0}, grad={1}\".format(y_val, grad))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Example 2: least squares\n",
    "\n",
    "Say that we have a linear model \n",
    "\n",
    "$$f(\\mathbf{x}) = \\theta \\cdot \\mathbf{x} = \\theta_1 x_1 + \\theta_2 x_2 + ... + \\theta_d x_d$$\n",
    "\n",
    "for a data point $\\mathbf{x}_i \\in \\mathbb{R}^d$ and a parameter set $\\theta \\in \\mathbb{R}^d$.\n",
    "\n",
    "Now, given a single label $y_i \\in \\mathbb{R}$, we want to calcuate the $l_2$ loss:\n",
    "\n",
    "$$J(\\theta) = |f(\\mathbf{x}) - y|_2^2$$\n",
    "\n",
    "So what is the gradient of $J(\\theta)$?\n",
    "\n",
    "$$\\nabla J(\\theta) = 2\\mathbf{x}(\\mathbf{x} \\cdot \\mathbf{\\theta} - y)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def l2_loss_linear_func(theta, x, y):\n",
    "    \"\"\"\n",
    "    Function that returns the value and gradient for a given theta vector\n",
    "    @param theta: nd numpy array [theta_1, theta_2, ..., theta_n]\n",
    "    @param x: same dimension as theta\n",
    "    #param y: a real number\n",
    "    @out: f(\\theta), gradient of f(\\theta)\n",
    "    \"\"\"\n",
    "    # compute the value\n",
    "    y_val = np.power(np.dot(theta, x) - y, 2)\n",
    "\n",
    "    # compute the gradient\n",
    "    gradient = 2 * x * (np.dot(theta,x) - y)\n",
    "    return y_val, gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_val=1936, grad=[ -88 -176 -264 -352 -440]\n"
     ]
    }
   ],
   "source": [
    "theta = np.array([1,2,3,4,5])\n",
    "x_i = np.array([1,2,3,4,5])\n",
    "y_i = 99\n",
    "y_val, grad = l2_loss_linear_func(theta, x_i, y_i)\n",
    "print(\"y_val={0}, grad={1}\".format(y_val, grad))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Testing the gradient\n",
    "\n",
    "So far we have worked with relatively simple functions where it is straight-forward to compute the gradient. For more complex functions, the gradient computation can be notoriously difficult to debug and get right.\n",
    "\n",
    "It is very important to test if the gradient is correct. Recall the mathematical definition of the derivative as:\n",
    "$$ \\frac{\\partial}{\\partial \\theta} f(\\theta) = \\text{lim}_{\\epsilon \\to 0} \\frac{f(\\theta + \\epsilon) - f(\\theta - \\epsilon) }{2 \\epsilon}$$\n",
    "\n",
    "We can approximate the gradient (left hand side) using the equation on the right hand side by setting $\\epsilon$ to a small constant, say around $10^{âˆ’4}$.\n",
    "\n",
    "Now let's expand this method to deal with vector input $\\theta \\in \\mathbb{R}^d$. Let's say we want to verify out gradient at demension $i$ $(\\nabla f(\\theta))_i$. We can make use of one-hot vector $e_i$ in which all dimension except the $i$th are 0 and the $i$th dimension has a value of 1: \n",
    "\n",
    "$$e_i = \\begin{bmatrix} 0\\\\ 0\\\\ ...\\\\ 1\\\\ ... \\\\0 \\end{bmatrix}$$\n",
    "\n",
    "The gradient at $i$th dimension can be then approximated as \n",
    "\n",
    "$$ [\\nabla f(\\theta)]^{(i)} \\approx \\frac{f(\\theta + \\epsilon e_i)-f(\\theta - \\epsilon e_i)}{2 \\epsilon} $$\n",
    "\n",
    "We can then calculate the approximation for all dimension.\n",
    "\n",
    "\n",
    "More information about the gradient checker can be found at http://deeplearning.stanford.edu/tutorial/supervised/DebuggingGradientChecking/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def approxiate_gradient(f_theta, theta, epsilon=1e-5):\n",
    "    \"\"\"\n",
    "    Function that performs the gradient approximation using the above logic\n",
    "    @param: f_theta, a function that calculates f(theta) and gradient\n",
    "    @param: theta, a numpy array\n",
    "    @output: the approximated value of the gradient at theta\n",
    "    \"\"\"\n",
    "    n_dim = len(theta)\n",
    "    gradient = np.zeros(n_dim)\n",
    "    \n",
    "    # iterate through all n dimension\n",
    "    for i in range(n_dim):\n",
    "        # construct e_i\n",
    "        e_i = np.zeros(n_dim)\n",
    "        e_i[i] = 1\n",
    "        # calcualte f(theta+epsilon) and f(theta-epsilon)\n",
    "        f_plus, _ = f_theta(theta + e_i * epsilon)\n",
    "        f_minus, _ = f_theta(theta - e_i * epsilon)\n",
    "        # calculate the approximated gradient at dimension i\n",
    "        gradient[i] = (f_plus-f_minus)/(2*epsilon)\n",
    "    return gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use this function to verify that our gradient calculation in the two examples above is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient (calculated by hand):  [ 58 108]\n",
      "Gradient (approximated):  [ 58. 108.]\n"
     ]
    }
   ],
   "source": [
    "theta = np.array([2,3])\n",
    "_, grad = simple_vector_func(theta)\n",
    "grad_approx = approxiate_gradient(simple_vector_func, theta)\n",
    "print(\"Gradient (calculated by hand): \", grad)\n",
    "print(\"Gradient (approximated): \", grad_approx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient (calculated by hand):  [ -88 -176 -264 -352 -440]\n",
      "Gradient (approximated):  [ -88.00000003 -175.99999999 -264.00000002 -351.99999999 -440.00000001]\n"
     ]
    }
   ],
   "source": [
    "theta = np.array([1,2,3,4,5])\n",
    "x_i = np.array([1,2,3,4,5])\n",
    "y_i = 99\n",
    "y_val, grad = l2_loss_linear_func(theta, x_i, y_i)\n",
    "# here because l2_loss_linear_func also takes in x_i and y_i\n",
    "# we need to use lambda to wrap the value of x_i and y_i\n",
    "grad_approx = approxiate_gradient(lambda theta: l2_loss_linear_func(theta, x_i, y_i), theta)\n",
    "print(\"Gradient (calculated by hand): \", grad)\n",
    "print(\"Gradient (approximated): \", grad_approx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## References\n",
    "- DS-GA 1003 Machine Learning Spring 2020"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
