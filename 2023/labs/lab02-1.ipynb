{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DS-GA 1003, Machine Learning Spring 2023\n",
    "### Lab 2: Gradient Descent and Adaptive Learning Rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Gradient Descent\n",
    "\n",
    "From Lab 1 we know how to calculate and verify the gradients, let's code up a gradient decendent function.\n",
    "\n",
    "For simplicity, let's assume that we are using a fixed learning rate and our termination condition is terminating at a fixed number of iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(f_theta, theta_0, lr=1e-3, n_iteration=1000):\n",
    "    \"\"\"\n",
    "    Simple gradient descent optimization\n",
    "    @param f_theta: a function that returns f(x) and its gradient\n",
    "    @param theta_0: initial estimate\n",
    "    @param lr: learning rate\n",
    "    @param n_iteration: number of iterations\n",
    "    \"\"\"\n",
    "    # create history dictionary for tracking progress per iteration.\n",
    "    hist = {'theta': [], 'f_theta': []}\n",
    "    \n",
    "    # initialize theta_0\n",
    "    theta_i =  theta_0\n",
    "    \n",
    "    # loop over iterations\n",
    "    for i in range(n_iteration):\n",
    "        # calculate the gradient\n",
    "        f_val, f_grad = f_theta(theta_i)\n",
    "        \n",
    "        # save history\n",
    "        hist['theta'].append(theta_i)\n",
    "        hist['f_theta'].append(f_val)\n",
    "        \n",
    "        # update\n",
    "        theta_i = theta_i - lr * f_grad\n",
    "    return theta_i, hist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test our implementation on a very simple example:\n",
    "\n",
    "$$f(\\theta) = \\theta^2 - 2 \\theta + 1$$\n",
    "\n",
    "where $\\theta \\in \\mathbb{R}$ is a real number.\n",
    "\n",
    "We know that\n",
    "\n",
    "$$\\theta^* = \\text{argmin}_{\\theta} f(\\theta) = 1$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_quad = lambda theta: (np.power(theta,2)-2*theta+1, 2*theta-2)\n",
    "theta_init = np.random.randn()\n",
    "theta_star, hist = gradient_descent(f_quad, theta_init, lr=1e-2)\n",
    "print(\"Initial theta = {0}, Optimal theta_star = {1}\".format(theta_init,theta_star))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let visualize how gradient descent decreases the function value at each iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = np.arange(1000)\n",
    "plt.plot(t, hist['f_theta'])\n",
    "plt.xlabel(\"iteration number\")\n",
    "plt.ylabel(\"$f( \\theta )$\")\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = np.arange(1000)\n",
    "plt.semilogx(t, hist['f_theta'])\n",
    "plt.xlabel(\"iteration number\")\n",
    "plt.ylabel(\"$f( \\theta )$\")\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = np.arange(1000)\n",
    "plt.semilogx(t, hist['theta'])\n",
    "plt.xlabel(\"iteration number\")\n",
    "plt.ylabel(\"theta\")\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Impact of learning rate\n",
    "\n",
    "In the previous example, we use a fixed learning rate through all iterations. The choice of learning rate has a very significant impact on the result. In the code block below, we run the optimization using 4 different learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_list = [1, 0.1, 0.01, 0.001, 0.0001]\n",
    "theta_init = -10\n",
    "\n",
    "for lr in lr_list:\n",
    "    # run the optimizer\n",
    "    theta_star, hist = gradient_descent(f_quad, theta_init, lr)   \n",
    "    # plot the learning curve\n",
    "    plt.semilogx(np.arange(1000), hist['theta'], label=str(lr))\n",
    "    plt.xlabel(\"iteration number\")\n",
    "    plt.ylabel(\"theta\")\n",
    "    # report \n",
    "    print(\"lr = {0}, theta* = {1}, f(theta*) = {2}\".format(lr, hist[\"theta\"][-1], hist[\"f_theta\"][-1]))\n",
    "    \n",
    "plt.grid()\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the plot above, we know that gradient descent gives us bad results if the learning rate is too large/small. Can we improve the result by letting the algorithm to adjust the learning rate during each iteration?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent_armijo(f_theta, theta_0, lr=1e-3, n_iteration=1000):\n",
    "    \"\"\"\n",
    "    Simple gradient descent optimization with learning rate adjusted using Armijo's rule\n",
    "    @param f_theta: a function that returns f(x) and its gradient\n",
    "    @param theta_0: initial estimate\n",
    "    @param lr: learning rate\n",
    "    @param n_iteration: number of iterations\n",
    "    \"\"\"\n",
    "    # create history dictionary for tracking progress per iteration.\n",
    "    hist = {'lr':[], 'theta': [], 'f_theta': []}\n",
    "    \n",
    "    # initialize theta_0\n",
    "    theta_i =  theta_0\n",
    "    \n",
    "    # loop over iterations\n",
    "    for i in range(n_iteration):\n",
    "        # calculate the gradient\n",
    "        f_val, f_grad = f_theta(theta_i)\n",
    "        \n",
    "        # calculate the next theta with this gradient\n",
    "        theta_i_plus_1 = theta_i - lr * f_grad\n",
    "        f_val_next, _ = f_theta(theta_i_plus_1)\n",
    "        \n",
    "        # calculate the required improvement\n",
    "        if isinstance(f_grad, np.ndarray):\n",
    "            df_est = lr*f_grad.dot(f_grad)\n",
    "        else:\n",
    "            df_est = lr*f_grad*f_grad\n",
    "        \n",
    "        # if the improvement is significant, perform the update and increase learning rate\n",
    "        if (f_val_next < f_val - 0.5*df_est):\n",
    "            lr *= 2\n",
    "            accept = True\n",
    "        # if the improvement is insignificant, don't do the update and decrease learning rate\n",
    "        else:\n",
    "            lr /= 2 \n",
    "            accept = False\n",
    "        \n",
    "        if accept:\n",
    "            # do update\n",
    "            theta_i = theta_i - lr * f_grad\n",
    "            # save history\n",
    "            hist['theta'].append(theta_i)\n",
    "            hist['f_theta'].append(f_val)\n",
    "            hist['lr'].append(lr)\n",
    "    return theta_i, hist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see if the improved gradient descent can work for initial learning rate that are too large/small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_list = [1, 0.1, 0.01, 0.001, 0.0001]\n",
    "theta_init = -10\n",
    "\n",
    "for lr in lr_list:\n",
    "    # run the optimizer\n",
    "    theta_star, hist = gradient_descent_armijo(f_quad, theta_init, lr)   \n",
    "    # plot the learning curve\n",
    "    plt.semilogx(np.arange(len(hist['f_theta'])), hist['f_theta'], label=str(lr))\n",
    "    plt.xlabel(\"iteration number\")\n",
    "    plt.ylabel(\"f(theta)\")\n",
    "    # report \n",
    "    print(\"lr = {0}, theta* = {1}, f(theta*) = {2}\".format(lr, hist[\"theta\"][-1], hist[\"f_theta\"][-1]))\n",
    "    \n",
    "plt.grid()\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference:\n",
    "\n",
    "- DS-GA 1003 Machine Learning Spring 2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "ds1003",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "a828d8b314c2ab8242716b2db56511dd51a2128c2785bd3d198105dd44ca157b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
