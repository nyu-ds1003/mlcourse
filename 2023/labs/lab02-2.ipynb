{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DS-GA 1003, Machine Learning Spring 2023\n",
    "### Lab 2: Stochastic vs Batch vs Mini-Batch Gradient Descent\n",
    " - Implementation borrowed from https://github.com/bhattbhavesh91/gradient-descent-variants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-25T03:56:23.993097Z",
     "start_time": "2018-08-25T03:56:23.138209Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import time\n",
    "from scipy import stats \n",
    "from sklearn.datasets import make_regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timeit(method):\n",
    "    def timed(*args, **kw):\n",
    "        ts = time.time()\n",
    "        result = method(*args, **kw)\n",
    "        te = time.time()\n",
    "        if 'log_time' in kw:\n",
    "            name = kw.get('log_name', method.__name__.upper())\n",
    "            kw['log_time'][name] = int((te - ts) * 1000)\n",
    "        else:\n",
    "            print('%r  %2.2f ms' % \\\n",
    "                  (method.__name__, (te - ts) * 1000))\n",
    "        return result\n",
    "    return timed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_regression(n_samples=10000, \n",
    "                       n_features=1, \n",
    "                       n_informative=1, \n",
    "                       noise=20,\n",
    "                       random_state=2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = X.flatten()\n",
    "slope, intercept,_,_,_ = stats.linregress(x,y)\n",
    "print (slope)\n",
    "print (intercept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_fit = np.vectorize(lambda x: x * slope + intercept)\n",
    "plt.plot(x,y, 'o', alpha=0.5)\n",
    "grid = np.arange(-3,3,0.1)\n",
    "plt.plot(grid,best_fit(grid), '.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost Function & Gradients\n",
    "\n",
    "<h4> The equation for calculating cost function and gradients are as shown below. Please note the cost function is for Linear regression. For other algorithms the cost function will be different and the gradients would have  to be derived from the cost functions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<b>Cost</b>\n",
    "$$ J(\\theta) = 1/2m \\sum_{i=1}^{m} (h(\\theta)^{(i)} - y^{(i)})^2 $$\n",
    "\n",
    "<b>Gradient</b>\n",
    "\n",
    "$$ \\frac{\\partial J(\\theta)}{\\partial \\theta_j} = 1/m\\sum_{i=1}^{m}(h(\\theta^{(i)} - y^{(i)}).X_j^{(i)} $$\n",
    "\n",
    "<b>Gradients</b>\n",
    "$$ \\theta_0: = \\theta_0 -\\alpha . (1/m .\\sum_{i=1}^{m}(h(\\theta^{(i)} - y^{(i)}).X_0^{(i)}) $$\n",
    "\n",
    "$$ \\theta_1: = \\theta_1 -\\alpha . (1/m .\\sum_{i=1}^{m}(h(\\theta^{(i)} - y^{(i)}).X_1^{(i)}) $$\n",
    "\n",
    "$$ \\theta_2: = \\theta_2 -\\alpha . (1/m .\\sum_{i=1}^{m}(h(\\theta^{(i)} - y^{(i)}).X_2^{(i)}) $$\n",
    "\n",
    "$$ \\theta_j: = \\theta_j -\\alpha . (1/m .\\sum_{i=1}^{m}(h(\\theta^{(i)} - y^{(i)}).X_0^{(i)}) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_cost(theta,X,y):\n",
    "    m = len(y)\n",
    "    predictions = X.dot(theta)\n",
    "    cost = (1/2*m) * np.sum(np.square(predictions-y))\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-25T03:56:25.204130Z",
     "start_time": "2018-08-25T03:56:25.188272Z"
    }
   },
   "outputs": [],
   "source": [
    "@timeit\n",
    "def gradient_descent(X, y, theta, learning_rate=0.01, iterations=100):\n",
    "    m = len(y)\n",
    "    cost_history = np.zeros(iterations)\n",
    "    theta_history = np.zeros((iterations, 2))\n",
    "    for it in range(iterations):\n",
    "        prediction = np.dot(X, theta)\n",
    "        theta = theta - (1 / m) * learning_rate * (X.T.dot((prediction - y)))\n",
    "        theta_history[it, :] = theta.T\n",
    "        cost_history[it] = cal_cost(theta, X, y)\n",
    "    return theta, cost_history, theta_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Let's start with 1000 iterations and a learning rate of 0.05. Start with theta from a Gaussian distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-25T03:56:27.126960Z",
     "start_time": "2018-08-25T03:56:27.062811Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "lr = 0.05\n",
    "n_iter = 1000\n",
    "theta = np.random.randn(2, 1)\n",
    "X_b = np.c_[np.ones((len(X), 1)), X]\n",
    "theta, cost_history, theta_history = gradient_descent(X_b, y, theta, lr, n_iter)\n",
    "print(\"Theta0: {:0.3f},\\nTheta1:{:0.3f}\".format(theta[0][0], theta[1][0]))\n",
    "print(\"Final cost/MSE:  {:0.3f}\".format(cost_history[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Let's plot the cost history over iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-25T04:01:31.912400Z",
     "start_time": "2018-08-25T04:01:31.604459Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(12,8))\n",
    "ax.set_ylabel('J(Theta)')\n",
    "ax.set_xlabel('Iterations')\n",
    "_=ax.plot(range(n_iter),cost_history,'b.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> After around 60 iterations the cost is flat so the remaining iterations  are not needed or will not result in any further optimization. Let us zoom in till iteration 100 and see the curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-19T05:22:30.561842Z",
     "start_time": "2018-08-19T05:22:30.371532Z"
    }
   },
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(10,8))\n",
    "_=ax.plot(range(100),cost_history[:100],'b.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-19T05:23:17.900469Z",
     "start_time": "2018-08-19T05:23:17.882614Z"
    }
   },
   "outputs": [],
   "source": [
    "@timeit\n",
    "def stocashtic_gradient_descent(X, y, theta, learning_rate=0.01, iterations=10):\n",
    "    m = len(y)\n",
    "    cost_history = np.zeros(iterations)\n",
    "    for it in range(iterations):\n",
    "        cost = 0.0\n",
    "        for i in range(m):\n",
    "            rand_ind = np.random.randint(0, m)\n",
    "            X_i = X[rand_ind, :].reshape(1, X.shape[1])\n",
    "            y_i = y[rand_ind].reshape(1, 1)\n",
    "            prediction = np.dot(X_i, theta)\n",
    "            theta = theta - (1 / m) * learning_rate * (X_i.T.dot((prediction - y_i)))\n",
    "            cost += cal_cost(theta, X_i, y_i)\n",
    "        cost_history[it] = cost\n",
    "    return theta, cost_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-19T05:23:20.091678Z",
     "start_time": "2018-08-19T05:23:19.910817Z"
    }
   },
   "outputs": [],
   "source": [
    "lr = 0.05\n",
    "n_iter = 1000\n",
    "theta = np.random.randn(2, 1)\n",
    "X_b = np.c_[np.ones((len(X),1)),X]\n",
    "theta,cost_history = stocashtic_gradient_descent(X_b,y,theta,lr,n_iter)\n",
    "print(\"Theta0: {:0.3f},\\nTheta1:{:0.3f}\".format(theta[0][0], theta[1][0]))\n",
    "print(\"Final cost/MSE:  {:0.3f}\".format(cost_history[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-19T05:23:20.736824Z",
     "start_time": "2018-08-19T05:23:20.484570Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "ax.set_ylabel(\"{J(Theta)}\", rotation=0)\n",
    "ax.set_xlabel(\"{Iterations}\")\n",
    "theta = np.random.randn(2, 1)\n",
    "_ = ax.plot(range(n_iter), cost_history, \"b.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mini Batch Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-19T05:23:25.240288Z",
     "start_time": "2018-08-19T05:23:25.217657Z"
    }
   },
   "outputs": [],
   "source": [
    "@timeit\n",
    "def minibatch_gradient_descent(X,y,theta,learning_rate=0.01, iterations=10, batch_size =20):\n",
    "    m = len(y)\n",
    "    cost_history = np.zeros(iterations)\n",
    "    n_batches = int(m/batch_size)\n",
    "    for it in range(iterations):\n",
    "        cost =0.0\n",
    "        indices = np.random.permutation(m)\n",
    "        X = X[indices]\n",
    "        y = y[indices]\n",
    "        for i in range(0,m,batch_size):\n",
    "            X_i = X[i:i+batch_size]\n",
    "            y_i = y[i:i+batch_size]\n",
    "            X_i = np.c_[np.ones(len(X_i)),X_i]\n",
    "            prediction = np.dot(X_i,theta)\n",
    "            theta = theta -(1/m)*learning_rate*( X_i.T.dot((prediction - y_i)))\n",
    "            cost += cal_cost(theta,X_i,y_i)\n",
    "        cost_history[it]  = cost\n",
    "    return theta, cost_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-19T05:23:25.835249Z",
     "start_time": "2018-08-19T05:23:25.724421Z"
    }
   },
   "outputs": [],
   "source": [
    "lr = 0.05\n",
    "n_iter = 1000\n",
    "theta = np.random.randn(2, 1)\n",
    "theta,cost_history = minibatch_gradient_descent(X,y,theta,lr,n_iter)\n",
    "print('Theta0:          {:0.3f},\\nTheta1:          {:0.3f}'.format(theta[0][0],theta[1][0]))\n",
    "print('Final cost/MSE:  {:0.3f}'.format(cost_history[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-19T05:23:26.512988Z",
     "start_time": "2018-08-19T05:23:26.326207Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "ax.set_ylabel(\"{J(Theta)}\", rotation=0)\n",
    "ax.set_xlabel(\"{Iterations}\")\n",
    "theta = np.random.randn(2, 1)\n",
    "_ = ax.plot(range(n_iter), cost_history, \"b.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(10,8))\n",
    "_=ax.plot(range(100),cost_history[:100],'b.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference:\n",
    "\n",
    "- 'gradient-descent-variants-notebook.ipynb' developed by Bhavesh Bhatt\n",
    "    - Github page: https://github.com/bhattbhavesh91/gradient-descent-variants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "gist": {
   "data": {
    "description": "Gradient Descent-Python",
    "public": true
   },
   "id": ""
  },
  "hide_input": false,
  "kernelspec": {
   "display_name": "ds1003",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16 (main, Jan 11 2023, 10:02:19) \n[Clang 14.0.6 ]"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "279.233px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "461.183px",
    "left": "846.167px",
    "right": "138.333px",
    "top": "127px",
    "width": "559.667px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "a828d8b314c2ab8242716b2db56511dd51a2128c2785bd3d198105dd44ca157b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
